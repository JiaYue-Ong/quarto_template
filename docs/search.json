[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Jia Yue Ong, a student for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550: Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550: Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "docs/analysis/Python Final Project.html",
    "href": "docs/analysis/Python Final Project.html",
    "title": "If US States were individual countries, how do they compare?",
    "section": "",
    "text": "!pip install cenpy\nimport cenpy\nimport numpy as np\nnp.random.seed(42)\n\n\n\n\nMedian Household Income\nEducation Levels\nUnemployment rate\nLabour force participation rate\nPopulation\nPoverty rate\n\n\n# Finding data set\navailable = cenpy.explorer.available()\navailable\n\n# We use data from ACS 5 Year, Monthly Export and Import\n\n\nacs = available.filter(regex=\"^ACS\", axis=0)\navailable.filter(regex=\"^ACSDT5Y\", axis=0)\n\n\ncenpy.explorer.explain(\"ACSDT5Y2023\")\nacs = cenpy.remote.APIConnection('ACSDT5Y2023')\n\n\nacs.varslike(\n    pattern=\"B19019_001E\",\n    by=\"attributes\",\n).sort_index()\n\n\nacs_variables = [\n    \"NAME\",\n    \"GEO_ID\",\n    \"B19019_001E\", # Total Median Household Income\n    \"B06009_001E\", # Educational Attainment- Total\n    \"B06009_002E\", # Educational Attainment- Less than high school graduate\n    \"B06009_003E\", # Educational Attainment- High school graduate (equivalent)\n    \"B06009_004E\", # Educational Attainment- Some college or associate's degree\n    \"B06009_005E\", # Educational Attainment- Bachelor's degree or higher\n    \"B01003_001E\", # Total Population\n    \"B23025_001E\", # Total Population above 16\n    \"B23025_002E\", # Labor Force- Total\n    \"B23025_005E\", # Labour Force- Unemployed\n    \"B17020_001E\", # Population for whom poverty is determined\n    \"B17020_002E\", # Population below poverty level\n]\n\n#1. Median Household Income [x]\n#2. Education Levels [x]\n#3. Unemployment rate [x]\n#4. Labour force participation rate [x]\n#5. Population [x]\n#6. Poverty rate [x]\n\n\n\n# Filter out the data for state\nUSA_acs_data = acs.query(\n    cols=acs_variables,\n    geo_unit=\"state:*\",\n)\n\nUSA_acs_data.head()\n\n\nfor variable in acs_variables:\n    # Convert all variables EXCEPT for NAME\n    if variable not in (\"NAME\", \"GEO_ID\"):\n        USA_acs_data[variable] = USA_acs_data[variable].astype(float)\n\n\nUSA_acs_data = USA_acs_data.rename(\n    columns={\n        \"B19019_001E\": \"MedHHInc\", # Total Median Household Income\n        \"B06009_001E\": \"EducTotal\", # Educational Attainment- Total\n        \"B06009_002E\": \"EducBelowHighSch\", # Educational Attainment- Less than high school graduate\n        \"B06009_003E\": \"EducHighSch\", # Educational Attainment- High school graduate (equivalent)\n        \"B06009_004E\": \"EducAssoc\", # Educational Attainment- Some college or associate's degree\n        \"B06009_005E\": \"EducBach\", # Educational Attainment- Bachelor's degree or higher\n        \"B01003_001E\": \"TotalPop\", # Total Population\n        \"B23025_001E\": \"TotalPop16\", # Total Population above 16\n        \"B23025_002E\": \"LabForTotal\", # Labor Force- Total\n        \"B23025_005E\": \"Unemployed\", # Labour Force- Unemployed\n        \"B17020_001E\": \"PopPovertyDetermined\", # Population for whom poverty is determined\n        \"B17020_002E\": \"PovertyPop\", # Population below poverty level\n    }\n)\n\nFeature engineering\n\nUSA_acs_data['PctBach'] = USA_acs_data['EducBach']/USA_acs_data['EducTotal']\nUSA_acs_data['PovertyRate'] = USA_acs_data['PovertyPop']/USA_acs_data['PopPovertyDetermined']\nUSA_acs_data['UnemploymentRate'] = USA_acs_data['Unemployed']/USA_acs_data['LabForTotal']\nUSA_acs_data['LabForParticipationRate'] = USA_acs_data['LabForTotal']/USA_acs_data['TotalPop16']\n\n#Remove Puerto Rico\nUSA_acs_data = USA_acs_data[USA_acs_data['NAME'] != 'Puerto Rico']\n\n\n\n\nExports\n\ncenpy.explorer.explain(\"ITMONTHLYEXPORTSSTATENAICS\")\nUS_export = cenpy.remote.APIConnection('ITMONTHLYEXPORTSSTATENAICS')\n\n\nUS_export.variables\n\n\nUS_export.geographies['fips']\n\n\nUS_export_data = US_export.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"]:\n    US_export_data[variable] = US_export_data[variable].astype(float)\n\nUS_export_data\n\n\n# Filter data for December 2023\nUS_export_2023 = US_export_data[(US_export_data['YEAR'] == 2023) & (US_export_data['MONTH'] == 12)]\n\nImports\n\ncenpy.explorer.explain(\"ITMONTHLYIMPORTSSTATENAICS\")\nUS_import = cenpy.remote.APIConnection('ITMONTHLYIMPORTSSTATENAICS')\n\n\nUS_import.variables\n\n\nUS_import_data = US_import.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"]:\n    US_import_data[variable] = US_import_data[variable].astype(float)\n\nUS_import_data\n\n\n# Filter data for December 2023\nUS_import_2023 = US_import_data[(US_export_data['YEAR'] == 2023) & (US_import_data['MONTH'] == 12)]\n\nNet Export (Export-Import)\n\n# Join export and import data\nUS_netexport_2023 = US_export_2023.merge(US_import_2023[['US_STATE', 'GEN_VAL_YR']], on='US_STATE', how='left')\n\n# Net export\nUS_netexport_2023[\"netexport\"] = US_netexport_2023[\"ALL_VAL_YR\"]-US_netexport_2023[\"GEN_VAL_YR\"]\n\n\n# Create new column with states name\nstate_names = {\n    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n    'DC': 'District of Columbia'\n}\n\nUS_netexport_2023['STATE_NAME'] = US_netexport_2023['US_STATE'].map(state_names)\n\n# Filter states only\nUS_netexport_2023 = US_netexport_2023.dropna(subset=['STATE_NAME'])\n\n\n\n\n\n!pip install beaapi-0.0.2-py3-none-any.whl\n\nimport beaapi\n\n\nbeakey = '60AB5AC4-7ED2-481D-B559-BB6000F924ED'\n\n\n# List of data set available\nlist_of_sets = beaapi.get_data_set_list(beakey)\n# List of parameters\nlist_of_params = beaapi.get_parameter_list(beakey, 'Regional')\n# List of parameters values\nlist_of_param_vals = beaapi.get_parameter_values(beakey, 'Regional', 'LineCode',)\nlist_of_param_vals\n\n\nbea_tbl = beaapi.get_data(beakey, datasetname='Regional', GeoFips= 'STATE', LineCode= '1', TableName='SAGDP9N', Year='2023')\ndisplay(bea_tbl.head(5))\n\n\nbea_state_tbl = bea_tbl[bea_tbl['GeoName'].isin(state_names.values())]\nbea_state_tbl.rename(columns={'DataValue': 'REALGDP'}, inplace=True)\n\n\n\n\nThe last available data was 2021.\n\n!pip install sodapy\nimport pandas as pd\nfrom sodapy import Socrata\n\n\ncdc = Socrata(\"data.cdc.gov\", None)\nlife = cdc.get(\"it4f-frdc\", limit=2000)\nlife_df = pd.DataFrame.from_records(life)\nlife_df = life_df[life_df[\"sex\"]==\"Total\"]\nlife_df[\"leb\"] = life_df[\"leb\"].astype(float)\nlife_df.rename(columns={'leb': 'life_expectancy'}, inplace=True)\n\n\n\n\n\nlab_pdt = pd.read_csv('https://raw.githubusercontent.com/JiaYue-Ong/Python-Final-Project/refs/heads/main/labor-productivity.csv')\nlab_pdt.head()\n\n\nlab_pdt_2023 = lab_pdt[lab_pdt['Area'].isin(state_names.values())]\nlab_pdt_2023 = lab_pdt_2023[[\"Area\",\"2023\"]]\nlab_pdt_2023.rename(columns={'Area': 'State', '2023': 'Labor_Productivity_2023'}, inplace=True)"
  },
  {
    "objectID": "docs/analysis/Python Final Project.html#getting-the-data-with-api",
    "href": "docs/analysis/Python Final Project.html#getting-the-data-with-api",
    "title": "If US States were individual countries, how do they compare?",
    "section": "",
    "text": "!pip install cenpy\nimport cenpy\nimport numpy as np\nnp.random.seed(42)\n\n\n\n\nMedian Household Income\nEducation Levels\nUnemployment rate\nLabour force participation rate\nPopulation\nPoverty rate\n\n\n# Finding data set\navailable = cenpy.explorer.available()\navailable\n\n# We use data from ACS 5 Year, Monthly Export and Import\n\n\nacs = available.filter(regex=\"^ACS\", axis=0)\navailable.filter(regex=\"^ACSDT5Y\", axis=0)\n\n\ncenpy.explorer.explain(\"ACSDT5Y2023\")\nacs = cenpy.remote.APIConnection('ACSDT5Y2023')\n\n\nacs.varslike(\n    pattern=\"B19019_001E\",\n    by=\"attributes\",\n).sort_index()\n\n\nacs_variables = [\n    \"NAME\",\n    \"GEO_ID\",\n    \"B19019_001E\", # Total Median Household Income\n    \"B06009_001E\", # Educational Attainment- Total\n    \"B06009_002E\", # Educational Attainment- Less than high school graduate\n    \"B06009_003E\", # Educational Attainment- High school graduate (equivalent)\n    \"B06009_004E\", # Educational Attainment- Some college or associate's degree\n    \"B06009_005E\", # Educational Attainment- Bachelor's degree or higher\n    \"B01003_001E\", # Total Population\n    \"B23025_001E\", # Total Population above 16\n    \"B23025_002E\", # Labor Force- Total\n    \"B23025_005E\", # Labour Force- Unemployed\n    \"B17020_001E\", # Population for whom poverty is determined\n    \"B17020_002E\", # Population below poverty level\n]\n\n#1. Median Household Income [x]\n#2. Education Levels [x]\n#3. Unemployment rate [x]\n#4. Labour force participation rate [x]\n#5. Population [x]\n#6. Poverty rate [x]\n\n\n\n# Filter out the data for state\nUSA_acs_data = acs.query(\n    cols=acs_variables,\n    geo_unit=\"state:*\",\n)\n\nUSA_acs_data.head()\n\n\nfor variable in acs_variables:\n    # Convert all variables EXCEPT for NAME\n    if variable not in (\"NAME\", \"GEO_ID\"):\n        USA_acs_data[variable] = USA_acs_data[variable].astype(float)\n\n\nUSA_acs_data = USA_acs_data.rename(\n    columns={\n        \"B19019_001E\": \"MedHHInc\", # Total Median Household Income\n        \"B06009_001E\": \"EducTotal\", # Educational Attainment- Total\n        \"B06009_002E\": \"EducBelowHighSch\", # Educational Attainment- Less than high school graduate\n        \"B06009_003E\": \"EducHighSch\", # Educational Attainment- High school graduate (equivalent)\n        \"B06009_004E\": \"EducAssoc\", # Educational Attainment- Some college or associate's degree\n        \"B06009_005E\": \"EducBach\", # Educational Attainment- Bachelor's degree or higher\n        \"B01003_001E\": \"TotalPop\", # Total Population\n        \"B23025_001E\": \"TotalPop16\", # Total Population above 16\n        \"B23025_002E\": \"LabForTotal\", # Labor Force- Total\n        \"B23025_005E\": \"Unemployed\", # Labour Force- Unemployed\n        \"B17020_001E\": \"PopPovertyDetermined\", # Population for whom poverty is determined\n        \"B17020_002E\": \"PovertyPop\", # Population below poverty level\n    }\n)\n\nFeature engineering\n\nUSA_acs_data['PctBach'] = USA_acs_data['EducBach']/USA_acs_data['EducTotal']\nUSA_acs_data['PovertyRate'] = USA_acs_data['PovertyPop']/USA_acs_data['PopPovertyDetermined']\nUSA_acs_data['UnemploymentRate'] = USA_acs_data['Unemployed']/USA_acs_data['LabForTotal']\nUSA_acs_data['LabForParticipationRate'] = USA_acs_data['LabForTotal']/USA_acs_data['TotalPop16']\n\n#Remove Puerto Rico\nUSA_acs_data = USA_acs_data[USA_acs_data['NAME'] != 'Puerto Rico']\n\n\n\n\nExports\n\ncenpy.explorer.explain(\"ITMONTHLYEXPORTSSTATENAICS\")\nUS_export = cenpy.remote.APIConnection('ITMONTHLYEXPORTSSTATENAICS')\n\n\nUS_export.variables\n\n\nUS_export.geographies['fips']\n\n\nUS_export_data = US_export.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"]:\n    US_export_data[variable] = US_export_data[variable].astype(float)\n\nUS_export_data\n\n\n# Filter data for December 2023\nUS_export_2023 = US_export_data[(US_export_data['YEAR'] == 2023) & (US_export_data['MONTH'] == 12)]\n\nImports\n\ncenpy.explorer.explain(\"ITMONTHLYIMPORTSSTATENAICS\")\nUS_import = cenpy.remote.APIConnection('ITMONTHLYIMPORTSSTATENAICS')\n\n\nUS_import.variables\n\n\nUS_import_data = US_import.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"]:\n    US_import_data[variable] = US_import_data[variable].astype(float)\n\nUS_import_data\n\n\n# Filter data for December 2023\nUS_import_2023 = US_import_data[(US_export_data['YEAR'] == 2023) & (US_import_data['MONTH'] == 12)]\n\nNet Export (Export-Import)\n\n# Join export and import data\nUS_netexport_2023 = US_export_2023.merge(US_import_2023[['US_STATE', 'GEN_VAL_YR']], on='US_STATE', how='left')\n\n# Net export\nUS_netexport_2023[\"netexport\"] = US_netexport_2023[\"ALL_VAL_YR\"]-US_netexport_2023[\"GEN_VAL_YR\"]\n\n\n# Create new column with states name\nstate_names = {\n    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n    'DC': 'District of Columbia'\n}\n\nUS_netexport_2023['STATE_NAME'] = US_netexport_2023['US_STATE'].map(state_names)\n\n# Filter states only\nUS_netexport_2023 = US_netexport_2023.dropna(subset=['STATE_NAME'])\n\n\n\n\n\n!pip install beaapi-0.0.2-py3-none-any.whl\n\nimport beaapi\n\n\nbeakey = '60AB5AC4-7ED2-481D-B559-BB6000F924ED'\n\n\n# List of data set available\nlist_of_sets = beaapi.get_data_set_list(beakey)\n# List of parameters\nlist_of_params = beaapi.get_parameter_list(beakey, 'Regional')\n# List of parameters values\nlist_of_param_vals = beaapi.get_parameter_values(beakey, 'Regional', 'LineCode',)\nlist_of_param_vals\n\n\nbea_tbl = beaapi.get_data(beakey, datasetname='Regional', GeoFips= 'STATE', LineCode= '1', TableName='SAGDP9N', Year='2023')\ndisplay(bea_tbl.head(5))\n\n\nbea_state_tbl = bea_tbl[bea_tbl['GeoName'].isin(state_names.values())]\nbea_state_tbl.rename(columns={'DataValue': 'REALGDP'}, inplace=True)\n\n\n\n\nThe last available data was 2021.\n\n!pip install sodapy\nimport pandas as pd\nfrom sodapy import Socrata\n\n\ncdc = Socrata(\"data.cdc.gov\", None)\nlife = cdc.get(\"it4f-frdc\", limit=2000)\nlife_df = pd.DataFrame.from_records(life)\nlife_df = life_df[life_df[\"sex\"]==\"Total\"]\nlife_df[\"leb\"] = life_df[\"leb\"].astype(float)\nlife_df.rename(columns={'leb': 'life_expectancy'}, inplace=True)\n\n\n\n\n\nlab_pdt = pd.read_csv('https://raw.githubusercontent.com/JiaYue-Ong/Python-Final-Project/refs/heads/main/labor-productivity.csv')\nlab_pdt.head()\n\n\nlab_pdt_2023 = lab_pdt[lab_pdt['Area'].isin(state_names.values())]\nlab_pdt_2023 = lab_pdt_2023[[\"Area\",\"2023\"]]\nlab_pdt_2023.rename(columns={'Area': 'State', '2023': 'Labor_Productivity_2023'}, inplace=True)"
  },
  {
    "objectID": "docs/analysis/Python Final Project.html#data-wrangling",
    "href": "docs/analysis/Python Final Project.html#data-wrangling",
    "title": "If US States were individual countries, how do they compare?",
    "section": "2. Data Wrangling",
    "text": "2. Data Wrangling\nDependent variables: 1. GDP per capita 2. Life expectancy 3. Median household income 4. Education levels\nIndependent variables 1. Unemployment rate 2. Labour force participation rate 3. Labour Productivity Private non-farm 4. Population 5. Poverty rate 6. Net exports by state\n\n# All dataset\nUSA_acs_data\nUS_netexport_2023\nbea_state_tbl\nlife_df\nlab_pdt_2023\n\n\n# Join ACS and netexport\ndf1 = USA_acs_data.merge(US_netexport_2023[['STATE_NAME', 'netexport']], left_on='NAME', right_on='STATE_NAME', how='left').drop(\n    columns=[\"STATE_NAME\"]\n)\n\n# Join bea_state_tbl\ndf2 = df1.merge(bea_state_tbl[['GeoName', 'REALGDP']], left_on='NAME', right_on='GeoName', how='left').drop(\n    columns=[\"GeoName\"]\n)\n\n# Join life_df\ndf3 = df2.merge(life_df[['area', 'life_expectancy']], left_on='NAME', right_on='area', how='left').drop(\n    columns=[\"area\"]\n)\n\n# Join lab_pdt_2023\ndf4 = df3.merge(lab_pdt_2023, left_on='NAME', right_on='State', how='left').drop(\n    columns=[\"State\"]\n)\n\n# Create Real GDP per capita\ndf4[\"REALGDPpercapita\"] = df4[\"REALGDP\"]/df4[\"TotalPop\"]\n\n\n# Get geographies of US States\nimport pygris\nfrom pygris import states\nfrom pygris.utils import shift_geometry\n\n\nus = states(cb = True, resolution = \"20m\", year=2023)\nus_rescaled = shift_geometry(us)\nus_rescaled.plot()\n\n\n# Join the data to geography\nus_rescaled.head()\n\n\nus_rescaled_final = us_rescaled.merge(\n    df4,\n    left_on=[\"GEOID\"],\n    right_on=[\"state\"],\n).drop(\n    columns=[\"state\"]\n)\n\n# Convert CRS\nus_rescaled_final = us_rescaled_final.to_crs(\"EPSG:4326\")\n\n\nus_rescaled_final.columns"
  },
  {
    "objectID": "docs/analysis/Python Final Project.html#exploratory-plots",
    "href": "docs/analysis/Python Final Project.html#exploratory-plots",
    "title": "If US States were individual countries, how do they compare?",
    "section": "3. Exploratory Plots",
    "text": "3. Exploratory Plots\n\n3.1 Correlation Matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Create a list of all variables\nvariables = ['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita']\n\n# Create a list of selected variables for later analysis\nselected_variables = ['REALGDPpercapita','life_expectancy','MedHHInc','PctBach','UnemploymentRate','LabForParticipationRate', 'Labor_Productivity_2023', 'TotalPop', 'PovertyRate', 'netexport']\n\n# Calculate the correlation matrix\ncorr_matrix = us_rescaled_final[variables].corr()\n\n# Plot the correlation matrix using seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n3.2 Repeated Chart and Bubble Plot\n\nimport altair as alt\n\n\n# Setup the selection brush\nbrush = alt.selection_interval()\n\n# Repeated chart\n(\n    alt.Chart(us_rescaled_final)\n    .mark_circle()\n    .encode(\n        x=alt.X(alt.repeat(\"column\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        y=alt.Y(alt.repeat(\"row\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        color=alt.condition(\n            brush, \"NAME_x:N\", alt.value(\"lightgray\")\n        ),  # conditional color\n        tooltip=['NAME_x'] + variables\n    )\n    .properties(\n        width=200,\n        height=200,\n    )\n    .add_params(brush)\n    .repeat(  # repeat variables across rows and columns\n        row=variables,\n        column=variables,\n    )\n)\n\n\n# Define dropdown bindings for both x and y axes\ndropdown_x = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='X-axis column '\n)\ndropdown_y = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='Y-axis column '\n)\n\n# Create parameters for x and y axes\nxcol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_x\n)\nycol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_y\n)\n\nchart = alt.Chart(us_rescaled_final).mark_circle().encode(\n    x=alt.X('x:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    y=alt.Y('y:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    size='TotalPop:Q',\n    color='NAME_x:N',\n    tooltip=['NAME_x'] + variables  # Concatenate NAME_x with the existing variables list\n).transform_calculate(\n    x=f'datum[{xcol_param.name}]',\n    y=f'datum[{ycol_param.name}]'\n).add_params(\n    xcol_param,\n    ycol_param,\n).properties(width=800, height=800)\n\nchart\n\n\n# Define dropdown bindings for both x and y axes\ndropdown_x = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='X-axis column '\n)\ndropdown_y = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='Y-axis column '\n)\ndropdown_size = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='Bubble Size '\n)\n\n# Create parameters for x and y axes\nxcol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_x\n)\nycol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_y\n)\nsize_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_size\n)\n\nchart2 = alt.Chart(us_rescaled_final).mark_circle().encode(\n    x=alt.X('x:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    y=alt.Y('y:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    size=alt.Size('size:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    color='NAME_x:N',\n    tooltip=['NAME_x'] + variables  # Concatenate NAME_x with the existing variables list\n).transform_calculate(\n    x=f'datum[{xcol_param.name}]',\n    y=f'datum[{ycol_param.name}]',\n    size=f'datum[{size_param.name}]'\n).add_params(\n    xcol_param,\n    ycol_param,\n    size_param,\n).properties(width=800, height=800)\n\nchart2\n\n\n\n3.3 Map\n\npip install geopandas hvplot panel\n\n\nimport geopandas as gpd\nimport hvplot.pandas\nimport panel as pn\n\n\n# Convert from wide to long data\nus_rescaled_final_long = pd.melt(us_rescaled_final,\n                                 id_vars = ['STATEFP', 'STATENS', 'GEOIDFQ', 'GEOID', 'STUSPS', 'NAME_x', 'LSAD','ALAND', 'AWATER', 'geometry', 'NAME_y', 'GEO_ID'],\n                                 value_vars=['MedHHInc', 'EducTotal', 'EducBelowHighSch', 'EducHighSch', 'EducAssoc', 'EducBach', 'TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed', 'PopPovertyDetermined', 'PovertyPop', 'PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita']\n                                 )\n\n\nus_rescaled_final_long.hvplot(\n    c=\"value\",\n    dynamic=False,\n    width=1000,\n    height=1000,\n    geo=True,\n    cmap=\"viridis\",\n    groupby=\"variable\",\n    )"
  },
  {
    "objectID": "docs/analysis/Python Final Project.html#predictive-modelling",
    "href": "docs/analysis/Python Final Project.html#predictive-modelling",
    "title": "If US States were individual countries, how do they compare?",
    "section": "5. Predictive Modelling",
    "text": "5. Predictive Modelling\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\n\nselected_variables\n\n\nusa_modelling = us_rescaled_final[selected_variables + [\"geometry\"]].dropna()\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(usa_modelling, test_size=0.3, random_state=42)\n\n\n5.1 GDP per capita against variables\n\n# the target labels: log of sale price\ny_GDPtrain = np.log(train_set[\"REALGDPpercapita\"])\ny_GDPtest = np.log(test_set[\"REALGDPpercapita\"])\n\n\n# The features\nGDPfeature_cols = [\n    'life_expectancy',\n    'MedHHInc',\n    'PctBach',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_GDPtrain = train_set[GDPfeature_cols].values\nX_GDPtest = test_set[GDPfeature_cols].values\n\n\n# Make a random forest pipeline\nforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nGDPscores = cross_val_score(\n    forest_pipeline,\n    X_GDPtrain,\n    y_GDPtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", GDPscores)\nprint(\"Scores mean = \", GDPscores.mean())\nprint(\"Score std dev = \", GDPscores.std())\n\n\n# Fit on the training data\nforest_pipeline.fit(X_GDPtrain, y_GDPtrain)\n\n# What's the test score?\nforest_pipeline.score(X_GDPtest, y_GDPtest)\n\n\n# Extract the regressor from the pipeline\nforest_model = forest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportanceGDP = pd.DataFrame(\n    {\"Feature\": GDPfeature_cols, \"Importance\": forest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportanceGDP.hvplot.barh(x=\"Feature\", y=\"Importance\")\n\n\n\n5.2 Life Expectancy against variables\n\n# the target labels: log of sale price\ny_LEtrain = np.log(train_set[\"life_expectancy\"])\ny_LEtest = np.log(test_set[\"life_expectancy\"])\n\n\n# The features\nLEfeature_cols = [\n    'REALGDPpercapita',\n    'MedHHInc',\n    'PctBach',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_LEtrain = train_set[LEfeature_cols].values\nX_LEtest = test_set[LEfeature_cols].values\n\n\n# Make a random forest pipeline\nLEforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nLEscores = cross_val_score(\n    forest_pipeline,\n    X_LEtrain,\n    y_LEtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", LEscores)\nprint(\"Scores mean = \", LEscores.mean())\nprint(\"Score std dev = \", LEscores.std())\n\n\n# Fit on the training data\nLEforest_pipeline.fit(X_LEtrain, y_LEtrain)\n\n# What's the test score?\nLEforest_pipeline.score(X_LEtest, y_LEtest)\n\n\n# Extract the regressor from the pipeline\nLEforest_model = LEforest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportanceLE = pd.DataFrame(\n    {\"Feature\": LEfeature_cols, \"Importance\": LEforest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportanceLE.hvplot.barh(x=\"Feature\", y=\"Importance\")\n\n\n\n5.3 Median Household Income against variables\n\n# the target labels: log of sale price\ny_HHINCtrain = np.log(train_set[\"MedHHInc\"])\ny_HHINCtest = np.log(test_set[\"MedHHInc\"])\n\n\n# The features\nHHINCfeature_cols = [\n    'REALGDPpercapita',\n    'life_expectancy',\n    'PctBach',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_HHINCtrain = train_set[HHINCfeature_cols].values\nX_HHINCtest = test_set[HHINCfeature_cols].values\n\n\n# Make a random forest pipeline\nHHINCforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nHHINCscores = cross_val_score(\n    forest_pipeline,\n    X_HHINCtrain,\n    y_HHINCtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", HHINCscores)\nprint(\"Scores mean = \", HHINCscores.mean())\nprint(\"Score std dev = \", HHINCscores.std())\n\n\n# Fit on the training data\nHHINCforest_pipeline.fit(X_HHINCtrain, y_HHINCtrain)\n\n# What's the test score?\nHHINCforest_pipeline.score(X_HHINCtest, y_HHINCtest)\n\n\n# Extract the regressor from the pipeline\nHHINCforest_model = HHINCforest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportanceHHINC = pd.DataFrame(\n    {\"Feature\": HHINCfeature_cols, \"Importance\": HHINCforest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportanceHHINC.hvplot.barh(x=\"Feature\", y=\"Importance\")\n\n\n\n5.4 Education levels\n\n# the target labels: log of sale price\ny_PctBachtrain = np.log(train_set[\"PctBach\"])\ny_PctBachtest = np.log(test_set[\"PctBach\"])\n\n\n# The features\nPctBachfeature_cols = [\n    'REALGDPpercapita',\n    'life_expectancy',\n    'MedHHInc',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_PctBachtrain = train_set[PctBachfeature_cols].values\nX_PctBachtest = test_set[PctBachfeature_cols].values\n\n\n# Make a random forest pipeline\nPctBachforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nPctBachscores = cross_val_score(\n    forest_pipeline,\n    X_PctBachtrain,\n    y_PctBachtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", PctBachscores)\nprint(\"Scores mean = \", PctBachscores.mean())\nprint(\"Score std dev = \", PctBachscores.std())\n\n\n# Fit on the training data\nPctBachforest_pipeline.fit(X_PctBachtrain, y_PctBachtrain)\n\n# What's the test score?\nPctBachforest_pipeline.score(X_PctBachtest, y_PctBachtest)\n\n\n# Extract the regressor from the pipeline\nPctBachforest_model = PctBachforest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportancePctBach = pd.DataFrame(\n    {\"Feature\": PctBachfeature_cols, \"Importance\": PctBachforest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportancePctBach.hvplot.barh(x=\"Feature\", y=\"Importance\")"
  }
]