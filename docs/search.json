[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ong Jia Yue adventures in Master of Urban Spatial Analytics",
    "section": "",
    "text": "Hello everyone\n\n\nMy name is Ong Jia Yue, a Master of Urban Spatial Analytics at the University of Pennsylvania student. I am interested in applying spatial analysis and urban-related knowledge to public policy challenges.\n\n\nI graduated with a Bachelor of Science (Honours) in Philosophy, Politics and Economics from University College London.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Ong Jia Yue adventures in Master of Urban Spatial Analytics",
    "section": "",
    "text": "Hello everyone\n\n\nMy name is Ong Jia Yue, a Master of Urban Spatial Analytics at the University of Pennsylvania student. I am interested in applying spatial analysis and urban-related knowledge to public policy challenges.\n\n\nI graduated with a Bachelor of Science (Honours) in Philosophy, Politics and Economics from University College London.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/Python-Final-Project.html",
    "href": "analysis/Python-Final-Project.html",
    "title": "If US States were individual countries, how do they compare?",
    "section": "",
    "text": "!pip install cenpy\nimport cenpy\nimport numpy as np\nnp.random.seed(42)\n\n\n\n\nMedian Household Income\nEducation Levels\nUnemployment rate\nLabour force participation rate\nPopulation\nPoverty rate\n\n\n# Finding data set\navailable = cenpy.explorer.available()\navailable\n\n# We use data from ACS 5 Year, Monthly Export and Import\n\n\nacs = available.filter(regex=\"^ACS\", axis=0)\navailable.filter(regex=\"^ACSDT5Y\", axis=0)\n\n\ncenpy.explorer.explain(\"ACSDT5Y2023\")\nacs = cenpy.remote.APIConnection('ACSDT5Y2023')\n\n\nacs.varslike(\n    pattern=\"B19019_001E\",\n    by=\"attributes\",\n).sort_index()\n\n\nacs_variables = [\n    \"NAME\",\n    \"GEO_ID\",\n    \"B19019_001E\", # Total Median Household Income\n    \"B06009_001E\", # Educational Attainment- Total\n    \"B06009_002E\", # Educational Attainment- Less than high school graduate\n    \"B06009_003E\", # Educational Attainment- High school graduate (equivalent)\n    \"B06009_004E\", # Educational Attainment- Some college or associate's degree\n    \"B06009_005E\", # Educational Attainment- Bachelor's degree or higher\n    \"B01003_001E\", # Total Population\n    \"B23025_001E\", # Total Population above 16\n    \"B23025_002E\", # Labor Force- Total\n    \"B23025_005E\", # Labour Force- Unemployed\n    \"B17020_001E\", # Population for whom poverty is determined\n    \"B17020_002E\", # Population below poverty level\n]\n\n#1. Median Household Income [x]\n#2. Education Levels [x]\n#3. Unemployment rate [x]\n#4. Labour force participation rate [x]\n#5. Population [x]\n#6. Poverty rate [x]\n\n\n\n# Filter out the data for state\nUSA_acs_data = acs.query(\n    cols=acs_variables,\n    geo_unit=\"state:*\",\n)\n\nUSA_acs_data.head()\n\n\nfor variable in acs_variables:\n    # Convert all variables EXCEPT for NAME\n    if variable not in (\"NAME\", \"GEO_ID\"):\n        USA_acs_data[variable] = USA_acs_data[variable].astype(float)\n\n\nUSA_acs_data = USA_acs_data.rename(\n    columns={\n        \"B19019_001E\": \"MedHHInc\", # Total Median Household Income\n        \"B06009_001E\": \"EducTotal\", # Educational Attainment- Total\n        \"B06009_002E\": \"EducBelowHighSch\", # Educational Attainment- Less than high school graduate\n        \"B06009_003E\": \"EducHighSch\", # Educational Attainment- High school graduate (equivalent)\n        \"B06009_004E\": \"EducAssoc\", # Educational Attainment- Some college or associate's degree\n        \"B06009_005E\": \"EducBach\", # Educational Attainment- Bachelor's degree or higher\n        \"B01003_001E\": \"TotalPop\", # Total Population\n        \"B23025_001E\": \"TotalPop16\", # Total Population above 16\n        \"B23025_002E\": \"LabForTotal\", # Labor Force- Total\n        \"B23025_005E\": \"Unemployed\", # Labour Force- Unemployed\n        \"B17020_001E\": \"PopPovertyDetermined\", # Population for whom poverty is determined\n        \"B17020_002E\": \"PovertyPop\", # Population below poverty level\n    }\n)\n\nFeature engineering\n\nUSA_acs_data['PctBach'] = USA_acs_data['EducBach']/USA_acs_data['EducTotal']\nUSA_acs_data['PovertyRate'] = USA_acs_data['PovertyPop']/USA_acs_data['PopPovertyDetermined']\nUSA_acs_data['UnemploymentRate'] = USA_acs_data['Unemployed']/USA_acs_data['LabForTotal']\nUSA_acs_data['LabForParticipationRate'] = USA_acs_data['LabForTotal']/USA_acs_data['TotalPop16']\n\n#Remove Puerto Rico\nUSA_acs_data = USA_acs_data[USA_acs_data['NAME'] != 'Puerto Rico']\n\n\n\n\nExports\n\ncenpy.explorer.explain(\"ITMONTHLYEXPORTSSTATENAICS\")\nUS_export = cenpy.remote.APIConnection('ITMONTHLYEXPORTSSTATENAICS')\n\n\nUS_export.variables\n\n\nUS_export.geographies['fips']\n\n\nUS_export_data = US_export.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"]:\n    US_export_data[variable] = US_export_data[variable].astype(float)\n\nUS_export_data\n\n\n# Filter data for December 2023\nUS_export_2023 = US_export_data[(US_export_data['YEAR'] == 2023) & (US_export_data['MONTH'] == 12)]\n\nImports\n\ncenpy.explorer.explain(\"ITMONTHLYIMPORTSSTATENAICS\")\nUS_import = cenpy.remote.APIConnection('ITMONTHLYIMPORTSSTATENAICS')\n\n\nUS_import.variables\n\n\nUS_import_data = US_import.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"]:\n    US_import_data[variable] = US_import_data[variable].astype(float)\n\nUS_import_data\n\n\n# Filter data for December 2023\nUS_import_2023 = US_import_data[(US_export_data['YEAR'] == 2023) & (US_import_data['MONTH'] == 12)]\n\nNet Export (Export-Import)\n\n# Join export and import data\nUS_netexport_2023 = US_export_2023.merge(US_import_2023[['US_STATE', 'GEN_VAL_YR']], on='US_STATE', how='left')\n\n# Net export\nUS_netexport_2023[\"netexport\"] = US_netexport_2023[\"ALL_VAL_YR\"]-US_netexport_2023[\"GEN_VAL_YR\"]\n\n\n# Create new column with states name\nstate_names = {\n    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n    'DC': 'District of Columbia'\n}\n\nUS_netexport_2023['STATE_NAME'] = US_netexport_2023['US_STATE'].map(state_names)\n\n# Filter states only\nUS_netexport_2023 = US_netexport_2023.dropna(subset=['STATE_NAME'])\n\n\n\n\n\n!pip install beaapi-0.0.2-py3-none-any.whl\n\nimport beaapi\n\n\nbeakey = '60AB5AC4-7ED2-481D-B559-BB6000F924ED'\n\n\n# List of data set available\nlist_of_sets = beaapi.get_data_set_list(beakey)\n# List of parameters\nlist_of_params = beaapi.get_parameter_list(beakey, 'Regional')\n# List of parameters values\nlist_of_param_vals = beaapi.get_parameter_values(beakey, 'Regional', 'LineCode',)\nlist_of_param_vals\n\n\nbea_tbl = beaapi.get_data(beakey, datasetname='Regional', GeoFips= 'STATE', LineCode= '1', TableName='SAGDP9N', Year='2023')\ndisplay(bea_tbl.head(5))\n\n\nbea_state_tbl = bea_tbl[bea_tbl['GeoName'].isin(state_names.values())]\nbea_state_tbl.rename(columns={'DataValue': 'REALGDP'}, inplace=True)\n\n\n\n\nThe last available data was 2021.\n\n!pip install sodapy\nimport pandas as pd\nfrom sodapy import Socrata\n\n\ncdc = Socrata(\"data.cdc.gov\", None)\nlife = cdc.get(\"it4f-frdc\", limit=2000)\nlife_df = pd.DataFrame.from_records(life)\nlife_df = life_df[life_df[\"sex\"]==\"Total\"]\nlife_df[\"leb\"] = life_df[\"leb\"].astype(float)\nlife_df.rename(columns={'leb': 'life_expectancy'}, inplace=True)\n\n\n\n\n\nlab_pdt = pd.read_csv('https://raw.githubusercontent.com/JiaYue-Ong/Python-Final-Project/refs/heads/main/labor-productivity.csv')\nlab_pdt.head()\n\n\nlab_pdt_2023 = lab_pdt[lab_pdt['Area'].isin(state_names.values())]\nlab_pdt_2023 = lab_pdt_2023[[\"Area\",\"2023\"]]\nlab_pdt_2023.rename(columns={'Area': 'State', '2023': 'Labor_Productivity_2023'}, inplace=True)"
  },
  {
    "objectID": "analysis/Python-Final-Project.html#getting-the-data-with-api",
    "href": "analysis/Python-Final-Project.html#getting-the-data-with-api",
    "title": "If US States were individual countries, how do they compare?",
    "section": "",
    "text": "!pip install cenpy\nimport cenpy\nimport numpy as np\nnp.random.seed(42)\n\n\n\n\nMedian Household Income\nEducation Levels\nUnemployment rate\nLabour force participation rate\nPopulation\nPoverty rate\n\n\n# Finding data set\navailable = cenpy.explorer.available()\navailable\n\n# We use data from ACS 5 Year, Monthly Export and Import\n\n\nacs = available.filter(regex=\"^ACS\", axis=0)\navailable.filter(regex=\"^ACSDT5Y\", axis=0)\n\n\ncenpy.explorer.explain(\"ACSDT5Y2023\")\nacs = cenpy.remote.APIConnection('ACSDT5Y2023')\n\n\nacs.varslike(\n    pattern=\"B19019_001E\",\n    by=\"attributes\",\n).sort_index()\n\n\nacs_variables = [\n    \"NAME\",\n    \"GEO_ID\",\n    \"B19019_001E\", # Total Median Household Income\n    \"B06009_001E\", # Educational Attainment- Total\n    \"B06009_002E\", # Educational Attainment- Less than high school graduate\n    \"B06009_003E\", # Educational Attainment- High school graduate (equivalent)\n    \"B06009_004E\", # Educational Attainment- Some college or associate's degree\n    \"B06009_005E\", # Educational Attainment- Bachelor's degree or higher\n    \"B01003_001E\", # Total Population\n    \"B23025_001E\", # Total Population above 16\n    \"B23025_002E\", # Labor Force- Total\n    \"B23025_005E\", # Labour Force- Unemployed\n    \"B17020_001E\", # Population for whom poverty is determined\n    \"B17020_002E\", # Population below poverty level\n]\n\n#1. Median Household Income [x]\n#2. Education Levels [x]\n#3. Unemployment rate [x]\n#4. Labour force participation rate [x]\n#5. Population [x]\n#6. Poverty rate [x]\n\n\n\n# Filter out the data for state\nUSA_acs_data = acs.query(\n    cols=acs_variables,\n    geo_unit=\"state:*\",\n)\n\nUSA_acs_data.head()\n\n\nfor variable in acs_variables:\n    # Convert all variables EXCEPT for NAME\n    if variable not in (\"NAME\", \"GEO_ID\"):\n        USA_acs_data[variable] = USA_acs_data[variable].astype(float)\n\n\nUSA_acs_data = USA_acs_data.rename(\n    columns={\n        \"B19019_001E\": \"MedHHInc\", # Total Median Household Income\n        \"B06009_001E\": \"EducTotal\", # Educational Attainment- Total\n        \"B06009_002E\": \"EducBelowHighSch\", # Educational Attainment- Less than high school graduate\n        \"B06009_003E\": \"EducHighSch\", # Educational Attainment- High school graduate (equivalent)\n        \"B06009_004E\": \"EducAssoc\", # Educational Attainment- Some college or associate's degree\n        \"B06009_005E\": \"EducBach\", # Educational Attainment- Bachelor's degree or higher\n        \"B01003_001E\": \"TotalPop\", # Total Population\n        \"B23025_001E\": \"TotalPop16\", # Total Population above 16\n        \"B23025_002E\": \"LabForTotal\", # Labor Force- Total\n        \"B23025_005E\": \"Unemployed\", # Labour Force- Unemployed\n        \"B17020_001E\": \"PopPovertyDetermined\", # Population for whom poverty is determined\n        \"B17020_002E\": \"PovertyPop\", # Population below poverty level\n    }\n)\n\nFeature engineering\n\nUSA_acs_data['PctBach'] = USA_acs_data['EducBach']/USA_acs_data['EducTotal']\nUSA_acs_data['PovertyRate'] = USA_acs_data['PovertyPop']/USA_acs_data['PopPovertyDetermined']\nUSA_acs_data['UnemploymentRate'] = USA_acs_data['Unemployed']/USA_acs_data['LabForTotal']\nUSA_acs_data['LabForParticipationRate'] = USA_acs_data['LabForTotal']/USA_acs_data['TotalPop16']\n\n#Remove Puerto Rico\nUSA_acs_data = USA_acs_data[USA_acs_data['NAME'] != 'Puerto Rico']\n\n\n\n\nExports\n\ncenpy.explorer.explain(\"ITMONTHLYEXPORTSSTATENAICS\")\nUS_export = cenpy.remote.APIConnection('ITMONTHLYEXPORTSSTATENAICS')\n\n\nUS_export.variables\n\n\nUS_export.geographies['fips']\n\n\nUS_export_data = US_export.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"]:\n    US_export_data[variable] = US_export_data[variable].astype(float)\n\nUS_export_data\n\n\n# Filter data for December 2023\nUS_export_2023 = US_export_data[(US_export_data['YEAR'] == 2023) & (US_export_data['MONTH'] == 12)]\n\nImports\n\ncenpy.explorer.explain(\"ITMONTHLYIMPORTSSTATENAICS\")\nUS_import = cenpy.remote.APIConnection('ITMONTHLYIMPORTSSTATENAICS')\n\n\nUS_import.variables\n\n\nUS_import_data = US_import.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"]:\n    US_import_data[variable] = US_import_data[variable].astype(float)\n\nUS_import_data\n\n\n# Filter data for December 2023\nUS_import_2023 = US_import_data[(US_export_data['YEAR'] == 2023) & (US_import_data['MONTH'] == 12)]\n\nNet Export (Export-Import)\n\n# Join export and import data\nUS_netexport_2023 = US_export_2023.merge(US_import_2023[['US_STATE', 'GEN_VAL_YR']], on='US_STATE', how='left')\n\n# Net export\nUS_netexport_2023[\"netexport\"] = US_netexport_2023[\"ALL_VAL_YR\"]-US_netexport_2023[\"GEN_VAL_YR\"]\n\n\n# Create new column with states name\nstate_names = {\n    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n    'DC': 'District of Columbia'\n}\n\nUS_netexport_2023['STATE_NAME'] = US_netexport_2023['US_STATE'].map(state_names)\n\n# Filter states only\nUS_netexport_2023 = US_netexport_2023.dropna(subset=['STATE_NAME'])\n\n\n\n\n\n!pip install beaapi-0.0.2-py3-none-any.whl\n\nimport beaapi\n\n\nbeakey = '60AB5AC4-7ED2-481D-B559-BB6000F924ED'\n\n\n# List of data set available\nlist_of_sets = beaapi.get_data_set_list(beakey)\n# List of parameters\nlist_of_params = beaapi.get_parameter_list(beakey, 'Regional')\n# List of parameters values\nlist_of_param_vals = beaapi.get_parameter_values(beakey, 'Regional', 'LineCode',)\nlist_of_param_vals\n\n\nbea_tbl = beaapi.get_data(beakey, datasetname='Regional', GeoFips= 'STATE', LineCode= '1', TableName='SAGDP9N', Year='2023')\ndisplay(bea_tbl.head(5))\n\n\nbea_state_tbl = bea_tbl[bea_tbl['GeoName'].isin(state_names.values())]\nbea_state_tbl.rename(columns={'DataValue': 'REALGDP'}, inplace=True)\n\n\n\n\nThe last available data was 2021.\n\n!pip install sodapy\nimport pandas as pd\nfrom sodapy import Socrata\n\n\ncdc = Socrata(\"data.cdc.gov\", None)\nlife = cdc.get(\"it4f-frdc\", limit=2000)\nlife_df = pd.DataFrame.from_records(life)\nlife_df = life_df[life_df[\"sex\"]==\"Total\"]\nlife_df[\"leb\"] = life_df[\"leb\"].astype(float)\nlife_df.rename(columns={'leb': 'life_expectancy'}, inplace=True)\n\n\n\n\n\nlab_pdt = pd.read_csv('https://raw.githubusercontent.com/JiaYue-Ong/Python-Final-Project/refs/heads/main/labor-productivity.csv')\nlab_pdt.head()\n\n\nlab_pdt_2023 = lab_pdt[lab_pdt['Area'].isin(state_names.values())]\nlab_pdt_2023 = lab_pdt_2023[[\"Area\",\"2023\"]]\nlab_pdt_2023.rename(columns={'Area': 'State', '2023': 'Labor_Productivity_2023'}, inplace=True)"
  },
  {
    "objectID": "analysis/Python-Final-Project.html#data-wrangling",
    "href": "analysis/Python-Final-Project.html#data-wrangling",
    "title": "If US States were individual countries, how do they compare?",
    "section": "2. Data Wrangling",
    "text": "2. Data Wrangling\nDependent variables: 1. GDP per capita 2. Life expectancy 3. Median household income 4. Education levels\nIndependent variables 1. Unemployment rate 2. Labour force participation rate 3. Labour Productivity Private non-farm 4. Population 5. Poverty rate 6. Net exports by state\n\n# All dataset\nUSA_acs_data\nUS_netexport_2023\nbea_state_tbl\nlife_df\nlab_pdt_2023\n\n\n# Join ACS and netexport\ndf1 = USA_acs_data.merge(US_netexport_2023[['STATE_NAME', 'netexport']], left_on='NAME', right_on='STATE_NAME', how='left').drop(\n    columns=[\"STATE_NAME\"]\n)\n\n# Join bea_state_tbl\ndf2 = df1.merge(bea_state_tbl[['GeoName', 'REALGDP']], left_on='NAME', right_on='GeoName', how='left').drop(\n    columns=[\"GeoName\"]\n)\n\n# Join life_df\ndf3 = df2.merge(life_df[['area', 'life_expectancy']], left_on='NAME', right_on='area', how='left').drop(\n    columns=[\"area\"]\n)\n\n# Join lab_pdt_2023\ndf4 = df3.merge(lab_pdt_2023, left_on='NAME', right_on='State', how='left').drop(\n    columns=[\"State\"]\n)\n\n# Create Real GDP per capita\ndf4[\"REALGDPpercapita\"] = df4[\"REALGDP\"]/df4[\"TotalPop\"]\n\n\n# Get geographies of US States\nimport pygris\nfrom pygris import states\nfrom pygris.utils import shift_geometry\n\n\nus = states(cb = True, resolution = \"20m\", year=2023)\nus_rescaled = shift_geometry(us)\nus_rescaled.plot()\n\n\n# Join the data to geography\nus_rescaled.head()\n\n\nus_rescaled_final = us_rescaled.merge(\n    df4,\n    left_on=[\"GEOID\"],\n    right_on=[\"state\"],\n).drop(\n    columns=[\"state\"]\n)\n\n# Convert CRS\nus_rescaled_final = us_rescaled_final.to_crs(\"EPSG:4326\")\n\n\nus_rescaled_final.columns"
  },
  {
    "objectID": "analysis/Python-Final-Project.html#exploratory-plots",
    "href": "analysis/Python-Final-Project.html#exploratory-plots",
    "title": "If US States were individual countries, how do they compare?",
    "section": "3. Exploratory Plots",
    "text": "3. Exploratory Plots\n\n3.1 Correlation Matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Create a list of all variables\nvariables = ['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita']\n\n# Create a list of selected variables for later analysis\nselected_variables = ['REALGDPpercapita','life_expectancy','MedHHInc','PctBach','UnemploymentRate','LabForParticipationRate', 'Labor_Productivity_2023', 'TotalPop', 'PovertyRate', 'netexport']\n\n# Calculate the correlation matrix\ncorr_matrix = us_rescaled_final[variables].corr()\n\n# Plot the correlation matrix using seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n3.2 Repeated Chart and Bubble Plot\n\nimport altair as alt\n\n\n# Setup the selection brush\nbrush = alt.selection_interval()\n\n# Repeated chart\n(\n    alt.Chart(us_rescaled_final)\n    .mark_circle()\n    .encode(\n        x=alt.X(alt.repeat(\"column\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        y=alt.Y(alt.repeat(\"row\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n        color=alt.condition(\n            brush, \"NAME_x:N\", alt.value(\"lightgray\")\n        ),  # conditional color\n        tooltip=['NAME_x'] + variables\n    )\n    .properties(\n        width=200,\n        height=200,\n    )\n    .add_params(brush)\n    .repeat(  # repeat variables across rows and columns\n        row=variables,\n        column=variables,\n    )\n)\n\n\n# Define dropdown bindings for both x and y axes\ndropdown_x = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='X-axis column '\n)\ndropdown_y = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='Y-axis column '\n)\n\n# Create parameters for x and y axes\nxcol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_x\n)\nycol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_y\n)\n\nchart = alt.Chart(us_rescaled_final).mark_circle().encode(\n    x=alt.X('x:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    y=alt.Y('y:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    size='TotalPop:Q',\n    color='NAME_x:N',\n    tooltip=['NAME_x'] + variables  # Concatenate NAME_x with the existing variables list\n).transform_calculate(\n    x=f'datum[{xcol_param.name}]',\n    y=f'datum[{ycol_param.name}]'\n).add_params(\n    xcol_param,\n    ycol_param,\n).properties(width=800, height=800)\n\nchart\n\n\n# Define dropdown bindings for both x and y axes\ndropdown_x = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='X-axis column '\n)\ndropdown_y = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='Y-axis column '\n)\ndropdown_size = alt.binding_select(\n    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n    name='Bubble Size '\n)\n\n# Create parameters for x and y axes\nxcol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_x\n)\nycol_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_y\n)\nsize_param = alt.param(\n    value='MedHHInc',\n    bind=dropdown_size\n)\n\nchart2 = alt.Chart(us_rescaled_final).mark_circle().encode(\n    x=alt.X('x:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    y=alt.Y('y:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    size=alt.Size('size:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n    color='NAME_x:N',\n    tooltip=['NAME_x'] + variables  # Concatenate NAME_x with the existing variables list\n).transform_calculate(\n    x=f'datum[{xcol_param.name}]',\n    y=f'datum[{ycol_param.name}]',\n    size=f'datum[{size_param.name}]'\n).add_params(\n    xcol_param,\n    ycol_param,\n    size_param,\n).properties(width=800, height=800)\n\nchart2\n\n\n\n3.3 Map\n\npip install geopandas hvplot panel\n\n\nimport geopandas as gpd\nimport hvplot.pandas\nimport panel as pn\n\n\n# Convert from wide to long data\nus_rescaled_final_long = pd.melt(us_rescaled_final,\n                                 id_vars = ['STATEFP', 'STATENS', 'GEOIDFQ', 'GEOID', 'STUSPS', 'NAME_x', 'LSAD','ALAND', 'AWATER', 'geometry', 'NAME_y', 'GEO_ID'],\n                                 value_vars=['MedHHInc', 'EducTotal', 'EducBelowHighSch', 'EducHighSch', 'EducAssoc', 'EducBach', 'TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed', 'PopPovertyDetermined', 'PovertyPop', 'PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita']\n                                 )\n\n\nus_rescaled_final_long.hvplot(\n    c=\"value\",\n    dynamic=False,\n    width=1000,\n    height=1000,\n    geo=True,\n    cmap=\"viridis\",\n    groupby=\"variable\",\n    )"
  },
  {
    "objectID": "analysis/Python-Final-Project.html#predictive-modelling",
    "href": "analysis/Python-Final-Project.html#predictive-modelling",
    "title": "If US States were individual countries, how do they compare?",
    "section": "5. Predictive Modelling",
    "text": "5. Predictive Modelling\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Model selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# Pipelines\nfrom sklearn.pipeline import make_pipeline\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\n\nselected_variables\n\n\nusa_modelling = us_rescaled_final[selected_variables + [\"geometry\"]].dropna()\n\n\n# Split the data 70/30\ntrain_set, test_set = train_test_split(usa_modelling, test_size=0.3, random_state=42)\n\n\n5.1 GDP per capita against variables\n\n# the target labels: log of sale price\ny_GDPtrain = np.log(train_set[\"REALGDPpercapita\"])\ny_GDPtest = np.log(test_set[\"REALGDPpercapita\"])\n\n\n# The features\nGDPfeature_cols = [\n    'life_expectancy',\n    'MedHHInc',\n    'PctBach',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_GDPtrain = train_set[GDPfeature_cols].values\nX_GDPtest = test_set[GDPfeature_cols].values\n\n\n# Make a random forest pipeline\nforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nGDPscores = cross_val_score(\n    forest_pipeline,\n    X_GDPtrain,\n    y_GDPtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", GDPscores)\nprint(\"Scores mean = \", GDPscores.mean())\nprint(\"Score std dev = \", GDPscores.std())\n\n\n# Fit on the training data\nforest_pipeline.fit(X_GDPtrain, y_GDPtrain)\n\n# What's the test score?\nforest_pipeline.score(X_GDPtest, y_GDPtest)\n\n\n# Extract the regressor from the pipeline\nforest_model = forest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportanceGDP = pd.DataFrame(\n    {\"Feature\": GDPfeature_cols, \"Importance\": forest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportanceGDP.hvplot.barh(x=\"Feature\", y=\"Importance\")\n\n\n\n5.2 Life Expectancy against variables\n\n# the target labels: log of sale price\ny_LEtrain = np.log(train_set[\"life_expectancy\"])\ny_LEtest = np.log(test_set[\"life_expectancy\"])\n\n\n# The features\nLEfeature_cols = [\n    'REALGDPpercapita',\n    'MedHHInc',\n    'PctBach',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_LEtrain = train_set[LEfeature_cols].values\nX_LEtest = test_set[LEfeature_cols].values\n\n\n# Make a random forest pipeline\nLEforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nLEscores = cross_val_score(\n    forest_pipeline,\n    X_LEtrain,\n    y_LEtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", LEscores)\nprint(\"Scores mean = \", LEscores.mean())\nprint(\"Score std dev = \", LEscores.std())\n\n\n# Fit on the training data\nLEforest_pipeline.fit(X_LEtrain, y_LEtrain)\n\n# What's the test score?\nLEforest_pipeline.score(X_LEtest, y_LEtest)\n\n\n# Extract the regressor from the pipeline\nLEforest_model = LEforest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportanceLE = pd.DataFrame(\n    {\"Feature\": LEfeature_cols, \"Importance\": LEforest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportanceLE.hvplot.barh(x=\"Feature\", y=\"Importance\")\n\n\n\n5.3 Median Household Income against variables\n\n# the target labels: log of sale price\ny_HHINCtrain = np.log(train_set[\"MedHHInc\"])\ny_HHINCtest = np.log(test_set[\"MedHHInc\"])\n\n\n# The features\nHHINCfeature_cols = [\n    'REALGDPpercapita',\n    'life_expectancy',\n    'PctBach',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_HHINCtrain = train_set[HHINCfeature_cols].values\nX_HHINCtest = test_set[HHINCfeature_cols].values\n\n\n# Make a random forest pipeline\nHHINCforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nHHINCscores = cross_val_score(\n    forest_pipeline,\n    X_HHINCtrain,\n    y_HHINCtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", HHINCscores)\nprint(\"Scores mean = \", HHINCscores.mean())\nprint(\"Score std dev = \", HHINCscores.std())\n\n\n# Fit on the training data\nHHINCforest_pipeline.fit(X_HHINCtrain, y_HHINCtrain)\n\n# What's the test score?\nHHINCforest_pipeline.score(X_HHINCtest, y_HHINCtest)\n\n\n# Extract the regressor from the pipeline\nHHINCforest_model = HHINCforest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportanceHHINC = pd.DataFrame(\n    {\"Feature\": HHINCfeature_cols, \"Importance\": HHINCforest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportanceHHINC.hvplot.barh(x=\"Feature\", y=\"Importance\")\n\n\n\n5.4 Education levels\n\n# the target labels: log of sale price\ny_PctBachtrain = np.log(train_set[\"PctBach\"])\ny_PctBachtest = np.log(test_set[\"PctBach\"])\n\n\n# The features\nPctBachfeature_cols = [\n    'REALGDPpercapita',\n    'life_expectancy',\n    'MedHHInc',\n    'UnemploymentRate',\n    'LabForParticipationRate',\n    'Labor_Productivity_2023',\n    'TotalPop',\n    'PovertyRate',\n    'netexport',\n]\nX_PctBachtrain = train_set[PctBachfeature_cols].values\nX_PctBachtest = test_set[PctBachfeature_cols].values\n\n\n# Make a random forest pipeline\nPctBachforest_pipeline = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 10-fold cross validation\nPctBachscores = cross_val_score(\n    forest_pipeline,\n    X_PctBachtrain,\n    y_PctBachtrain,\n    cv=10,\n)\n\n# Report\nprint(\"R^2 scores = \", PctBachscores)\nprint(\"Scores mean = \", PctBachscores.mean())\nprint(\"Score std dev = \", PctBachscores.std())\n\n\n# Fit on the training data\nPctBachforest_pipeline.fit(X_PctBachtrain, y_PctBachtrain)\n\n# What's the test score?\nPctBachforest_pipeline.score(X_PctBachtest, y_PctBachtest)\n\n\n# Extract the regressor from the pipeline\nPctBachforest_model = PctBachforest_pipeline[\"randomforestregressor\"]\n\n\n# Create the data frame of importances\nimportancePctBach = pd.DataFrame(\n    {\"Feature\": PctBachfeature_cols, \"Importance\": PctBachforest_model.feature_importances_}\n).sort_values(\"Importance\")\n\n\nimportancePctBach.hvplot.barh(x=\"Feature\", y=\"Importance\")"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "analysis/1.5-Conclusion.html",
    "href": "analysis/1.5-Conclusion.html",
    "title": "5. Conclusion",
    "section": "",
    "text": "Our analysis shows that there are large variations in median household income, education levels, unemployment rate, labour force participation rate, labour productivity, population, poverty rate across US states. There are moderate variations in life expectancy, with all states having life expectancy greater than 70. There is small variations in GDP per capita and net exports across states.\nOur analysis suggest suggest that states concentrated mostly in the Southern parts of the USA are more deprived, especially those in Cluster 1. The other clusters appear to be relatively more developed economically based on the 10 variables.\nPolicymakers can thus consider improving their economic conditions through further investigation whether there is a causal relationship between the four variables and their predictors. For example, there is likely to be a causal relationship between higher percentage of bachelor’s degree graduate and median household income, as a bachelor’s degree provides more opportunity to access higher wage and skills jobs. On the other hand, higher life expectancy is unlikely to cause a higher percentage of bachelor’s degree graduate. Future research and analysis can also look into using states as a fixed-effect variable to model for variations in socioeconomic variables.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "5. Conclusion"
    ]
  },
  {
    "objectID": "analysis/1.3-Clustering.html",
    "href": "analysis/1.3-Clustering.html",
    "title": "3. Clustering",
    "section": "",
    "text": "4. Cluster Analysis\nWe can apply K-means clustering to try and identify which states are more developed/deprived than others. Clustering is an unsupervised classification of patterns (i.e., GDP per capita and life expectancy across states) into similar groups (clusters), and k-means does this by classifying the states into k-number of groups.\nSince our data set has values of different magnitude, we first standardise our variables.\nWe then run the kneed package to find an optimal number of k clusters.\n\n\n6\n\n\nRun the k-means clustering with the optimal k value. We can visualise the results in the table and map below.\n\n\n\n\n\n\n\n\n\nlabel\nsize\n\n\n\n\n0\n0\n16\n\n\n1\n1\n9\n\n\n2\n2\n1\n\n\n3\n3\n14\n\n\n4\n4\n10\n\n\n5\n5\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlabel\nREALGDPpercapita\nlife_expectancy\nMedHHInc\nPctBach\nUnemploymentRate\nLabForParticipationRate\nLabor_Productivity_2023\nTotalPop\nPovertyRate\nnetexport\n\n\n\n\n0\n0\n63641.287918\n77.162500\n79378.875000\n0.217191\n0.041852\n0.649131\n104.720938\n1.963924e+06\n0.108652\n-1.437363e+09\n\n\n1\n1\n48835.305886\n72.233333\n60957.555556\n0.164726\n0.053548\n0.584285\n105.058889\n3.697484e+06\n0.166678\n-9.561401e+08\n\n\n2\n2\n217272.523022\n75.300000\n106287.000000\n0.260746\n0.064270\n0.719794\n114.806000\n6.720790e+05\n0.145306\n1.553795e+08\n\n\n3\n3\n63293.022605\n75.364286\n73875.857143\n0.201638\n0.051788\n0.628321\n107.645143\n1.219532e+07\n0.128928\n-4.489356e+10\n\n\n4\n4\n73713.193650\n78.180000\n93235.800000\n0.244063\n0.043336\n0.672959\n114.374100\n5.703807e+06\n0.093520\n-1.743753e+10\n\n\n5\n5\n82783.538426\n78.300000\n96334.000000\n0.224029\n0.063654\n0.638570\n118.074000\n3.924278e+07\n0.119664\n-2.715042e+11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nStates in Cluster 1 appear to be the worst off economically across most of the 10 variables compared to other clusters. Cluster 3 has the lowest mean value in real GDP per capita, life expectancy, median household income, percentage of bachelor’s degree, labour participation rate, and highest mean value in poverty rate.\nClusters 2 and 5 have only 1 state, District of Columbia and California respectively. Both clusters have high median household income and real GDP per capita, and labor productivity\nOne limitation of cluster is its unsupervised nature. This means that it is possible for the clusters to be unrelated to development/deprivation. Nonetheless, the results from our table suggest that the classification are related to level of economic development/deprivation.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "3. Clustering"
    ]
  },
  {
    "objectID": "analysis/1.2-Exploratory_Plots.html",
    "href": "analysis/1.2-Exploratory_Plots.html",
    "title": "2. Exploratory Plots",
    "section": "",
    "text": "3.1 Correlation Matrix\nWe create a correlation matrix to explore how the variables are statistically related. For exploratory purposes, I included variables that were used in intermediate processing steps, in addition to the 10 variables of interest for our modelling.\n\n\n\n\n\n\n\n\n\nWe observe some interesting correlations. Real GDP has high positive correlation with population, size of labour force and number of employed. When these variables are converted into ratios such as labour force participation rate and unemployment rate, the correlation becomes weaker.\nThere are some reasonable correlation expectations. For example, percentage of bachelor’s degree graduates has a high negative correlation with poverty rate and a high positive correlation with median household income, labor force participation rate and life expectancy. Meanwhile. poverty rate has high negative correlation with median household income, labor force paritcipation rate and life expecatancy.\n\n\n3.2 Repeated Chart and Bubble Plot\nWe can visualise these relationships further using line charts. I first create a repeated chart for all the variables. From an aesthetic perspective, the visualization can be improved further.\n\n\n\n\n\n\n\n\nWe can improve the visualisation by creating an interactive bubble plot inspired by Gapminder: https://www.gapminder.org/tools/#$chart-type=bubbles&url=v2. This allows us to select the variables we are interested in and see their distribution",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "2. Exploratory Plots"
    ]
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Jia Yue Ong, a student for the course.\nYou can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2024.\nWrite something about you\n\nor about something you like"
  },
  {
    "objectID": "analysis/1.1-Get_Data.html",
    "href": "analysis/1.1-Get_Data.html",
    "title": "1. Getting data and data processing",
    "section": "",
    "text": "For our analysis, we need to collect data on the following variables.\nDependent variables: 1. GDP per capita 2. Life expectancy 3. Median household income 4. Education levels (we use percentage of population with bachelor’s degree as reference)\nThe dependent variables were selected because they are key measures related to the Human Development Index.\nIndependent variables: 1. Unemployment rate 2. Labour force participation rate 3. Labour Productivity (private non-farm) 4. Population 5. Poverty rate 6. Net exports by state\nI also use the dependent variables as independent variables in the analysis. For example, when modelling the relation ship of GDP per capita against socioeconomic variables, life expectancy, median household income, and education levels become independent variables. This would enable us to explore which variables is more important in predicting the dependent variable.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "1. Getting data and data processing"
    ]
  },
  {
    "objectID": "analysis/1.1-Get_Data.html#getting-the-data-with-api",
    "href": "analysis/1.1-Get_Data.html#getting-the-data-with-api",
    "title": "1. Getting data and data processing",
    "section": "1. Getting the data with API",
    "text": "1. Getting the data with API\n\n\nCode\n!pip install cenpy\nimport cenpy\nimport numpy as np\nnp.random.seed(42)\n\n\n\n1.1 ACS 5-Year Estimate by US States\nFrom cenpy we can access American Community Survey 5-Year data to obtain/derive the following socioeconomic variables: 1. Median Household Income 2. Education Levels 3. Unemployment rate 4. Labour force participation rate 5. Population 6. Poverty rate\n\n\nCode\n# Finding data set\navailable = cenpy.explorer.available()\navailable.head(n=5)\n\n# We use data from ACS 5 Year, Monthly Export and Import\n\n\n\n\n\n\n\n\n\nc_isTimeseries\ntemporal\nspatial\nc_isAggregate\npublisher\nreferences\nprogramCode\nmodified\nlicense\nkeyword\n...\ndescription\nbureauCode\naccessLevel\ntitle\nc_isAvailable\nc_isCube\nc_isMicrodata\nc_documentationLink\nc_dataset\nvintage\n\n\n\n\nABSCB2017\nNaN\n2017/2017\nUnited States\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:007\n2020-04-30 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe Annual Business Survey (ABS) provides info...\n006:07\npublic\nAnnual Business Survey: Characteristics of Bus...\nTrue\nNaN\nNaN\nhttps://www.census.gov/developer/\n(abscb,)\n2017.0\n\n\nABSCB2018\nNaN\n2018/2018\nUnited States\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:007\n2020-10-26 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe Annual Business Survey (ABS) provides info...\n006:07\npublic\nAnnual Business Survey: Characteristics of Bus...\nTrue\nNaN\nNaN\nhttps://www.census.gov/developer/\n(abscb,)\n2018.0\n\n\nABSCB2019\nNaN\n2019/2019\nUS\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:007\n2021-08-17 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe Annual Business Survey (ABS) provides info...\n006:07\npublic\n2019 Annual Business Survey: Characteristics o...\nTrue\nNaN\nNaN\nhttps://www.census.gov/developer/\n(abscb,)\n2019.0\n\n\nABSCB2020\nNaN\n2020/2020\nUS\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:007\n2022-08-03 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe Annual Business Survey (ABS) provides info...\n006:07\npublic\n2020 Annual Business Survey: Characteristics o...\nTrue\nNaN\nNaN\nhttps://www.census.gov/developer/\n(abscb,)\n2020.0\n\n\nABSCB2021\nNaN\n2021/2021\nUnited States\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:007\n2023-07-24 10:30:52.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe Annual Business Survey (ABS) provides info...\n006:07\npublic\n2021 Annual Business Survey: Characteristics o...\nTrue\nNaN\nNaN\nhttps://www.census.gov/developer/\n(abscb,)\n2021.0\n\n\n\n\n5 rows × 23 columns\n\n\n\nWe identify the dataset we want to use from cenpy. In this case it is ACSDT5Y.\n\n\nCode\nacs = available.filter(regex=\"^ACS\", axis=0)\navailable.filter(regex=\"^ACSDT5Y\", axis=0).head(n=5)\n\n\n\n\n\n\n\n\n\nc_isTimeseries\ntemporal\nspatial\nc_isAggregate\npublisher\nreferences\nprogramCode\nmodified\nlicense\nkeyword\n...\ndescription\nbureauCode\naccessLevel\ntitle\nc_isAvailable\nc_isCube\nc_isMicrodata\nc_documentationLink\nc_dataset\nvintage\n\n\n\n\nACSDT5Y2009\nNaN\nNaN\nNaN\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:004\n2019-08-27 13:11:18.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe American Community Survey (ACS) is an ongo...\n006:07\npublic\nAmerican Community Survey: 5-Year Estimates: D...\nTrue\nTrue\nNaN\nhttps://www.census.gov/developer/\n(acs, acs5)\n2009.0\n\n\nACSDT5Y2010\nNaN\nNaN\nUnited States\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:004\n2018-07-04 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe American Community Survey (ACS) is an ongo...\n006:07\npublic\nACS 5-Year Detailed Tables\nTrue\nTrue\nNaN\nhttps://www.census.gov/developer/\n(acs, acs5)\n2010.0\n\n\nACSDT5Y2011\nNaN\nNaN\nNaN\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:004\n2018-07-04 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe American Community Survey (ACS) is an ongo...\n006:07\npublic\nACS 5-Year Detailed Tables\nTrue\nTrue\nNaN\nhttps://www.census.gov/developer/\n(acs, acs5)\n2011.0\n\n\nACSDT5Y2012\nNaN\nNaN\nNaN\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:004\n2018-07-04 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe American Community Survey (ACS) is an ongo...\n006:07\npublic\nACS 5-Year Detailed Tables\nTrue\nTrue\nNaN\nhttps://www.census.gov/developer/\n(acs, acs5)\n2012.0\n\n\nACSDT5Y2013\nNaN\nNaN\nNaN\nTrue\nU.S. Census Bureau\nhttps://www.census.gov/developers/\n006:004\n2018-07-04 00:00:00.0\nhttps://creativecommons.org/publicdomain/zero/...\n(census,)\n...\nThe American Community Survey (ACS) is an ongo...\n006:07\npublic\nACS 5-Year Detailed Tables\nTrue\nTrue\nNaN\nhttps://www.census.gov/developer/\n(acs, acs5)\n2013.0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n\nCode\ncenpy.explorer.explain(\"ACSDT5Y2023\")\nacs = cenpy.remote.APIConnection('ACSDT5Y2023')\n\n\n\n\nCode\nacs.varslike(\n    pattern=\"B19019_001E\",\n    by=\"attributes\",\n).sort_index()\n\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nattributes\nrequired\n\n\n\n\nB19019_001E\nEstimate!!Total:\nMedian Household Income in the Past 12 Months ...\nint\nB19019\n0\nNaN\nNaN\nB19019_001EA,B19019_001M,B19019_001MA\nNaN\n\n\n\n\n\n\n\nWe then extract out the following variables. Since the variables are in object format, we will convert them to float. Rename the variables for clarity.\n\n\nCode\nacs_variables = [\n    \"NAME\",\n    \"GEO_ID\",\n    \"B19019_001E\", # Total Median Household Income\n    \"B06009_001E\", # Educational Attainment- Total\n    \"B06009_002E\", # Educational Attainment- Less than high school graduate\n    \"B06009_003E\", # Educational Attainment- High school graduate (equivalent)\n    \"B06009_004E\", # Educational Attainment- Some college or associate's degree\n    \"B06009_005E\", # Educational Attainment- Bachelor's degree or higher\n    \"B01003_001E\", # Total Population\n    \"B23025_001E\", # Total Population above 16\n    \"B23025_002E\", # Labor Force- Total\n    \"B23025_005E\", # Labour Force- Unemployed\n    \"B17020_001E\", # Population for whom poverty is determined\n    \"B17020_002E\", # Population below poverty level\n]\n\n\n\n\n\n\nCode\n# Filter out the data for state\nUSA_acs_data = acs.query(\n    cols=acs_variables,\n    geo_unit=\"state:*\",\n)\n\nUSA_acs_data.head(n=5)\n\n\n\n\n\n\n\n\n\nNAME\nGEO_ID\nB19019_001E\nB06009_001E\nB06009_002E\nB06009_003E\nB06009_004E\nB06009_005E\nB01003_001E\nB23025_001E\nB23025_002E\nB23025_005E\nB17020_001E\nB17020_002E\nstate\n\n\n\n\n0\nAlabama\n0400000US01\n62027\n3448302\n409381\n1046374\n1035466\n584986\n5054253\n4056609\n2358667\n112849\n4913932\n768185\n01\n\n\n1\nAlaska\n0400000US02\n89336\n487903\n31854\n141206\n162411\n95793\n733971\n574021\n380935\n20644\n716703\n72978\n02\n\n\n2\nArizona\n0400000US04\n76872\n4990633\n544528\n1171693\n1646950\n999535\n7268175\n5862117\n3547314\n182184\n7109159\n907125\n04\n\n\n3\nArkansas\n0400000US05\n58773\n2043779\n233857\n702778\n593923\n324625\n3032651\n2409758\n1405609\n71269\n2944742\n471783\n05\n\n\n4\nCalifornia\n0400000US06\n96334\n26941198\n4149146\n5496195\n7461496\n6035609\n39242785\n31545603\n20144078\n1282259\n38529452\n4610600\n06\n\n\n\n\n\n\n\n\n\nCode\nfor variable in acs_variables:\n    # Convert all variables EXCEPT for NAME\n    if variable not in (\"NAME\", \"GEO_ID\"):\n        USA_acs_data[variable] = USA_acs_data[variable].astype(float)\n\n\n\n\nCode\nUSA_acs_data = USA_acs_data.rename(\n    columns={\n        \"B19019_001E\": \"MedHHInc\", # Total Median Household Income\n        \"B06009_001E\": \"EducTotal\", # Educational Attainment- Total\n        \"B06009_002E\": \"EducBelowHighSch\", # Educational Attainment- Less than high school graduate\n        \"B06009_003E\": \"EducHighSch\", # Educational Attainment- High school graduate (equivalent)\n        \"B06009_004E\": \"EducAssoc\", # Educational Attainment- Some college or associate's degree\n        \"B06009_005E\": \"EducBach\", # Educational Attainment- Bachelor's degree or higher\n        \"B01003_001E\": \"TotalPop\", # Total Population\n        \"B23025_001E\": \"TotalPop16\", # Total Population above 16\n        \"B23025_002E\": \"LabForTotal\", # Labor Force- Total\n        \"B23025_005E\": \"Unemployed\", # Labour Force- Unemployed\n        \"B17020_001E\": \"PopPovertyDetermined\", # Population for whom poverty is determined\n        \"B17020_002E\": \"PovertyPop\", # Population below poverty level\n    }\n)\n\n\nFeature engineering:\nWe apply feature engineering to derive the following socioeconomic variables: 1) education levels, 2) poverty rate, 3) unemployment rate, and 4) labour force participation rate.\n\n\nCode\nUSA_acs_data['PctBach'] = USA_acs_data['EducBach']/USA_acs_data['EducTotal']\nUSA_acs_data['PovertyRate'] = USA_acs_data['PovertyPop']/USA_acs_data['PopPovertyDetermined']\nUSA_acs_data['UnemploymentRate'] = USA_acs_data['Unemployed']/USA_acs_data['LabForTotal']\nUSA_acs_data['LabForParticipationRate'] = USA_acs_data['LabForTotal']/USA_acs_data['TotalPop16']\n\n#Remove Puerto Rico\nUSA_acs_data = USA_acs_data[USA_acs_data['NAME'] != 'Puerto Rico']\n\n\n\n\n1.2 International Trade: Net Export\nExports:\nFrom cenpy we can also access USA trade data to calculate the net exports by state.\n\n\nCode\ncenpy.explorer.explain(\"ITMONTHLYEXPORTSSTATENAICS\")\nUS_export = cenpy.remote.APIConnection('ITMONTHLYEXPORTSSTATENAICS')\n\n\nThe code below shows the list of variables in the export dataset and the geographic display level.\n\n\nCode\nUS_export.variables\n\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nrequired\ndatetime\nattributes\nvalues\n\n\n\n\nfor\nCensus API FIPS 'for' clause\nCensus API Geography Specification\nfips-for\nN/A\n0\nTrue\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nin\nCensus API FIPS 'in' clause\nCensus API Geography Specification\nfips-in\nN/A\n0\nTrue\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nucgid\nUniform Census Geography Identifier clause\nCensus API Geography Specification\nucgid\nN/A\n0\nTrue\nTrue\nNaN\nNaN\nNaN\nNaN\n\n\ntime\nISO-8601 Date/Time value\nCensus API Date/Time Specification\ndatetime\nN/A\n0\nTrue\nNaN\ntrue\n{'year': True, 'month': True}\nNaN\nNaN\n\n\nCNT_WGT_YR\nYear-to-Date Containerized Vessel Shipping Weight\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMONTH\nMonth\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSUMMARY_LVL\nDetail or Country Grouping indicator\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCOMM_LVL\n4-character aggregation levels for commodity code\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nCOMM_LVL_LABEL\nNaN\n\n\nCNT_VAL_MO\nContainerized Vessel Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_WGT_MO\nAir Shipping Weight\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nVES_WGT_MO\nVessel Shipping Weight\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUSITC\nUSITC Standard Countries and Areas\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNAICS\nNAICS-based International Trade code\nMonthly Exports by State of Origin and NAICS code\nstring\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\ndefault displayed\nNaN\nNAICS_LABEL\n{'item': {'-': 'Total For All North American I...\n\n\nCNT_VAL_YR\nYear-to-Date Containerized Vessel Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nGEOCOMP\nGEO_ID Component\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNAICS_LDESC\n150-character NAICS-based International Trade ...\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nYEAR\nYear\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSUMMARY_LVL2\nVariables being summarized\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_WGT_YR\nYear-to-Date Air Shipping Weight\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCNT_WGT_MO\nContainerized Vessel Shipping Weight\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nVES_WGT_YR\nYear-to-Date Vessel Shipping Weight\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUSITCHISTORY\nUSITC Standard Historical Countries and Areas\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nWORLD\nWorld\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUSITCREG\nUSITC Standard International Regions\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nALL_VAL_MO\nTotal Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_VAL_MO\nAir Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nALL_VAL_YR\nYear-to-Date Total Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUS_STATE\nState of Origin of Movement\nMonthly Exports by State of Origin and NAICS code\nstring\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nUS_STATE_LABEL\nNaN\n\n\nGEO_ID\nGeographic identifier code\nMonthly Exports by State of Origin and NAICS code\nstring\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNAME\nNaN\n\n\nSUMLEVEL\nSummary Level code\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nLAST_UPDATE\nDate of Last Update\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNAICS_SDESC\n50-character NAICS-based International Trade C...\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nVES_VAL_MO\nVessel Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCTY_CODE\nCountry Code\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nCTY_NAME\nNaN\n\n\nVES_VAL_YR\nYear-to-Date Vessel Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_VAL_YR\nYear-to-Date Air Value\nMonthly Exports by State of Origin and NAICS code\nint\nIT00EXPORTSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nCode\nUS_export.geographies['fips']\n\n\n\n\n\n\n\n\n\nname\ngeoLevelDisplay\nreferenceDate\n\n\n\n\n0\nworld\nW01\n2022-01-01\n\n\n1\nusitc standard international regions\nW04\n2022-01-01\n\n\n2\nusitc standard countries and areas\nW16\n2022-01-01\n\n\n3\nusitc standard historical countries and areas\nW20\n2022-01-01\n\n\n\n\n\n\n\nWe run the code below to extract out the total trade to date per month in 2023. We set geographic display levels to world since we are interested only in aggregate exports.\n\n\nCode\nUS_export_data = US_export.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"]:\n    US_export_data[variable] = US_export_data[variable].astype(float)\n\nUS_export_data.head(n=5)\n\n\n\n\n\n\n\n\n\nUS_STATE\nGEO_ID\nYEAR\nMONTH\nALL_VAL_YR\nworld\n\n\n\n\n0\n-\nW0100Y1WO\n2013.0\n1.0\n1.230319e+11\n1\n\n\n1\nAK\nW0100Y1WO\n2013.0\n1.0\n7.526427e+07\n1\n\n\n2\nAL\nW0100Y1WO\n2013.0\n1.0\n1.300844e+09\n1\n\n\n3\nAR\nW0100Y1WO\n2013.0\n1.0\n4.963303e+08\n1\n\n\n4\nAZ\nW0100Y1WO\n2013.0\n1.0\n1.510832e+09\n1\n\n\n\n\n\n\n\nFinally, filter for December to get aggregate exports in 2023\n\n\nCode\n# Filter data for December 2023\nUS_export_2023 = US_export_data[(US_export_data['YEAR'] == 2023) & (US_export_data['MONTH'] == 12)]\n\n\nImports:\nThe code below shows the list of variables in the import dataset and the geographic display level.\n\n\nCode\ncenpy.explorer.explain(\"ITMONTHLYIMPORTSSTATENAICS\")\nUS_import = cenpy.remote.APIConnection('ITMONTHLYIMPORTSSTATENAICS')\n\n\n\n\nCode\nUS_import.variables\n\n\n\n\n\n\n\n\n\nlabel\nconcept\npredicateType\ngroup\nlimit\npredicateOnly\nhasGeoCollectionSupport\nrequired\ndatetime\nattributes\nvalues\n\n\n\n\nfor\nCensus API FIPS 'for' clause\nCensus API Geography Specification\nfips-for\nN/A\n0\nTrue\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nin\nCensus API FIPS 'in' clause\nCensus API Geography Specification\nfips-in\nN/A\n0\nTrue\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nucgid\nUniform Census Geography Identifier clause\nCensus API Geography Specification\nucgid\nN/A\n0\nTrue\nTrue\nNaN\nNaN\nNaN\nNaN\n\n\ntime\nISO-8601 Date/Time value\nCensus API Date/Time Specification\ndatetime\nN/A\n0\nTrue\nNaN\ntrue\n{'month': True, 'year': True}\nNaN\nNaN\n\n\nCNT_WGT_YR\nYear-to-Date Containerized Vessel Shipping Weight\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nMONTH\nMonth\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSUMMARY_LVL\nDetail or Country Grouping indicator\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCOMM_LVL\n4-character aggregation levels for commodity code\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nCOMM_LVL_LABEL\nNaN\n\n\nCON_VAL_MO\nImports for Consumption, Total Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCNT_VAL_MO\nContainerized Vessel Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nVES_WGT_MO\nVessel Shipping Weight\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_WGT_MO\nAir Shipping Weight\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUSITC\nUSITC Standard Countries and Areas\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNAICS\nNAICS-based International Trade Code\nMonthly Imports by State and NAICS\nstring\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\ndefault displayed\nNaN\nNAICS_LABEL\n{'item': {'-': 'Total For All North American I...\n\n\nCON_VAL_YR\nYear-to-Date Imports for Consumption, Total Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCNT_VAL_YR\nYear-to-Date Containerized Vessel Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nGEOCOMP\nGEO_ID Component\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nGEN_VAL_YR\nYear-to-Date General Imports, Total Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNAICS_LDESC\n150-character NAICS-based International Trade ...\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nYEAR\nYear\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSUMMARY_LVL2\nVariables being summarized\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_WGT_YR\nYear-to-Date Air Shipping Weight\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCNT_WGT_MO\nContainerized Vessel Shipping Weight\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nVES_WGT_YR\nYear-to-Date Vessel Shipping Weight\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUSITCHISTORY\nUSITC Standard Historical Countries and Areas\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nGEN_VAL_MO\nGeneral Imports, Total Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nWORLD\nWorld\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUSITCREG\nUSITC Standard International Regions\nNaN\nNaN\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_VAL_MO\nAir Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nUS_STATE\nState of Destination\nMonthly Imports by State and NAICS\nstring\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nUS_STATE_LABEL\nNaN\n\n\nGEO_ID\nGeographic identifier code\nMonthly Imports by State and NAICS\nstring\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNAME\nNaN\n\n\nSUMLEVEL\nSummary Level code\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nLAST_UPDATE\nDate of Last Update\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nNAICS_SDESC\n50-character NAICS-based International Trade C...\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nVES_VAL_MO\nVessel Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCTY_CODE\nCountry Code\nNaN\nstring\nN/A\n0\nNaN\nNaN\nNaN\nNaN\nCTY_NAME\nNaN\n\n\nVES_VAL_YR\nYear-to-Date Vessel Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAIR_VAL_YR\nYear-to-Date Air Value\nMonthly Imports by State and NAICS\nint\nIT00IMPORTSSTATENAICS\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe run the code below to extract out the total trade to date per month in 2023. We set geographic display levels to world since we are interested only in aggregate imports.\n\n\nCode\nUS_import_data = US_import.query(\n    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"],\n    geo_unit=\"world:*\",\n    )\n\nfor variable in [\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"]:\n    US_import_data[variable] = US_import_data[variable].astype(float)\n\nUS_import_data.head(n=5)\n\n\n\n\n\n\n\n\n\nUS_STATE\nGEO_ID\nYEAR\nMONTH\nGEN_VAL_YR\nworld\n\n\n\n\n0\n-\nW0100Y1WO\n2013.0\n1.0\n1.851686e+11\n1\n\n\n1\nAK\nW0100Y1WO\n2013.0\n1.0\n7.511713e+07\n1\n\n\n2\nAL\nW0100Y1WO\n2013.0\n1.0\n1.525358e+09\n1\n\n\n3\nAR\nW0100Y1WO\n2013.0\n1.0\n6.097465e+08\n1\n\n\n4\nAZ\nW0100Y1WO\n2013.0\n1.0\n1.782983e+09\n1\n\n\n\n\n\n\n\nFinally, filter for December to get aggregate import in 2023\n\n\nCode\n# Filter data for December 2023\nUS_import_2023 = US_import_data[(US_import_data['YEAR'] == 2023) & (US_import_data['MONTH'] == 12)]\n\n\nNet Export (Export-Import)\nNet export is then calculated by taking Export – Import. Rename the states to include their full name for joining later.\n\n\nCode\n# Join export and import data\nUS_netexport_2023 = US_export_2023.merge(US_import_2023[['US_STATE', 'GEN_VAL_YR']], on='US_STATE', how='left')\n\n# Net export\nUS_netexport_2023[\"netexport\"] = US_netexport_2023[\"ALL_VAL_YR\"]-US_netexport_2023[\"GEN_VAL_YR\"]\n\n\n\n\nCode\n# Create new column with states name\nstate_names = {\n    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n    'DC': 'District of Columbia'\n}\n\nUS_netexport_2023['STATE_NAME'] = US_netexport_2023['US_STATE'].map(state_names)\n\n# Filter states only\nUS_netexport_2023 = US_netexport_2023.dropna(subset=['STATE_NAME'])\n\n\n\n\n1.3 Bureau of Economic Analysis: Gross Domesitc Product (GDP)\nFrom Bureau of Economic Analysis, we can access real GDP data to calculate real GDP per capita for each state. Real GDP per capital is total real GDP divided by total population, with total population data coming from ACS 5-Year Survey earlier.\nFirst, import the package beaapi.\n\n\nCode\n!pip install beaapi-0.0.2-py3-none-any.whl\n\nimport beaapi\n\n\nSet up your API key. You will need to request one from the BEA website if your do not have and validate it: https://www.bea.gov/resources/for-developers\n\n\nCode\nimport config\nbeakey = config.bea_api_key #API Key\n\n\nReview the list of dataset and parameters available. For this analysis we will be using ‘Regional’ dataset and SAGDP9N.\n\n\nCode\n# List of data set available\nlist_of_sets = beaapi.get_data_set_list(beakey)\n# List of parameters\nlist_of_params = beaapi.get_parameter_list(beakey, 'Regional')\n# List of parameters values\nlist_of_param_vals = beaapi.get_parameter_values(beakey, 'Regional', 'LineCode',)\nlist_of_param_vals.head(n=5)\n\n\n\n\n\n\n\n\n\nKey\nDesc\n\n\n\n\n0\n1\n[CAGDP1] Real Gross Domestic Product (GDP)\n\n\n1\n1\n[CAGDP11] Contributions to percent change in r...\n\n\n2\n1\n[CAGDP2] Gross Domestic Product (GDP): All ind...\n\n\n3\n1\n[CAGDP8] Chain-type quantity indexes for real ...\n\n\n4\n1\n[CAGDP9] Real GDP: All industry total\n\n\n\n\n\n\n\nExtract the real GDP for each state using the code below.\n\n\nCode\nbea_tbl = beaapi.get_data(beakey, datasetname='Regional', GeoFips= 'STATE', LineCode= '1', TableName='SAGDP9N', Year='2023')\ndisplay(bea_tbl.head(5))\n\n\n\n\n\n\n\n\n\nCode\nGeoFips\nGeoName\nTimePeriod\nCL_UNIT\nUNIT_MULT\nDataValue\n\n\n\n\n0\nSAGDP9N-1\n00000\nUnited States\n2023\nMillions of chained 2017 dollars\n6\n22671096.0\n\n\n1\nSAGDP9N-1\n01000\nAlabama\n2023\nMillions of chained 2017 dollars\n6\n245354.7\n\n\n2\nSAGDP9N-1\n02000\nAlaska\n2023\nMillions of chained 2017 dollars\n6\n54059.7\n\n\n3\nSAGDP9N-1\n04000\nArizona\n2023\nMillions of chained 2017 dollars\n6\n422399.6\n\n\n4\nSAGDP9N-1\n05000\nArkansas\n2023\nMillions of chained 2017 dollars\n6\n142860.6\n\n\n\n\n\n\n\n\n\nCode\nbea_state_tbl = bea_tbl[bea_tbl['GeoName'].isin(state_names.values())]\nbea_state_tbl.rename(columns={'DataValue': 'REALGDP'}, inplace=True)\n\n\n\n\n1.4 Centers for Disease Control and Prevention: Life Expectancy by state\nThe most recent data available for Life Expectancy was for 2021 and this data was published in 2024. We assume for our analysis that 2021 and 2024 life expectancy are similar.\nInstall and import the packages pandas and Socrata.\n\n\nCode\n!pip install sodapy\nimport pandas as pd\nfrom sodapy import Socrata\n\n\nGet the data of life expectancy for each state and both sexes using the code below. This allows us to access the data on https://data.cdc.gov/NCHS/U-S-State-Life-Expectancy-by-Sex-2021/it4f-frdc/about_data\n\n\nCode\ncdc = Socrata(\"data.cdc.gov\", None)\nlife = cdc.get(\"it4f-frdc\", limit=2000)\nlife_df = pd.DataFrame.from_records(life)\nlife_df = life_df[life_df[\"sex\"]==\"Total\"]\nlife_df[\"leb\"] = life_df[\"leb\"].astype(float)\nlife_df.rename(columns={'leb': 'life_expectancy'}, inplace=True)\n\n\nWARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n\n\n\n\n1.5 Bureau of Labor Statistics: Labor Productivity\nI downloaded the csv file of labour productivity from the Bureau of Labor Statistics website https://www.bls.gov/productivity/tables/. The data are all indexed with reference to 2017, with 2017=100 for all states. This means that by comparing index values in 2023, we can see which states have become relatively more productive than others since 2017. The code below gets labor productivity data for 2023.\n\n\nCode\nlab_pdt = pd.read_csv('https://raw.githubusercontent.com/JiaYue-Ong/Python-Final-Project/refs/heads/main/labor-productivity.csv')\nlab_pdt.head()\n\n\n\n\n\n\n\n\n\nSector\nArea\nBasis\nMeasure\nUnits\n2007\n2008\n2009\n2010\n2011\n...\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\n\n\n\n\n0\nPrivate nonfarm\nAlabama\nAll workers\nLabor productivity\nIndex (2017=100)\n89.822\n93.420\n97.944\n100.873\n102.157\n...\n99.612\n98.811\n98.859\n100.0\n101.089\n102.665\n106.346\n107.076\n106.151\n107.992\n\n\n1\nPrivate nonfarm\nAlaska\nAll workers\nLabor productivity\nIndex (2017=100)\n92.779\n92.841\n106.275\n99.772\n94.378\n...\n95.217\n97.048\n98.582\n100.0\n95.548\n92.722\n100.851\n99.282\n93.051\n100.303\n\n\n2\nPrivate nonfarm\nArizona\nAll workers\nLabor productivity\nIndex (2017=100)\n94.611\n94.863\n92.812\n95.264\n98.879\n...\n98.800\n97.511\n98.976\n100.0\n101.491\n103.499\n108.574\n113.423\n109.700\n109.179\n\n\n3\nPrivate nonfarm\nArkansas\nAll workers\nLabor productivity\nIndex (2017=100)\n94.132\n94.271\n96.365\n100.418\n100.516\n...\n103.521\n102.602\n102.102\n100.0\n100.728\n101.374\n107.309\n107.508\n105.726\n106.947\n\n\n4\nPrivate nonfarm\nCalifornia\nAll workers\nLabor productivity\nIndex (2017=100)\n83.296\n85.382\n89.426\n91.852\n91.820\n...\n93.341\n95.603\n96.438\n100.0\n102.159\n107.356\n117.005\n120.095\n115.262\n118.074\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n\nCode\nlab_pdt_2023 = lab_pdt[lab_pdt['Area'].isin(state_names.values())]\nlab_pdt_2023 = lab_pdt_2023[[\"Area\",\"2023\"]]\nlab_pdt_2023.rename(columns={'Area': 'State', '2023': 'Labor_Productivity_2023'}, inplace=True)",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "1. Getting data and data processing"
    ]
  },
  {
    "objectID": "analysis/1.1-Get_Data.html#data-processing",
    "href": "analysis/1.1-Get_Data.html#data-processing",
    "title": "1. Getting data and data processing",
    "section": "2. Data Processing",
    "text": "2. Data Processing\nWe now join all our dataset together into one dataframe, by the individual states. After joining, we calculate the Real GDP per capita for each state.\n\n\nCode\n# All dataset\nUSA_acs_data\nUS_netexport_2023\nbea_state_tbl\nlife_df\nlab_pdt_2023\n\n\n\n\nCode\n# Join ACS and netexport\ndf1 = USA_acs_data.merge(US_netexport_2023[['STATE_NAME', 'netexport']], left_on='NAME', right_on='STATE_NAME', how='left').drop(\n    columns=[\"STATE_NAME\"]\n)\n\n# Join bea_state_tbl\ndf2 = df1.merge(bea_state_tbl[['GeoName', 'REALGDP']], left_on='NAME', right_on='GeoName', how='left').drop(\n    columns=[\"GeoName\"]\n)\n\n# Join life_df\ndf3 = df2.merge(life_df[['area', 'life_expectancy']], left_on='NAME', right_on='area', how='left').drop(\n    columns=[\"area\"]\n)\n\n# Join lab_pdt_2023\ndf4 = df3.merge(lab_pdt_2023, left_on='NAME', right_on='State', how='left').drop(\n    columns=[\"State\"]\n)\n\n# Create Real GDP per capita\ndf4[\"REALGDPpercapita\"] = (df4[\"REALGDP\"]*10**6)/df4[\"TotalPop\"]\n\n\nWe then obtain a geographic map of USA and join the dataframe to the geographies. This would allow us to conduct spatial analysis\n\n\nCode\n# Get geographies of US States\nimport pygris\nfrom pygris import states\nfrom pygris.utils import shift_geometry\n\n\n\n\nCode\nus = states(cb = True, resolution = \"20m\", year=2023)\nus_rescaled = shift_geometry(us)\nus_rescaled.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Join the data to geography\nus_rescaled.head()\n\n\n\n\n\n\n\n\n\nSTATEFP\nSTATENS\nGEOIDFQ\nGEOID\nSTUSPS\nNAME\nLSAD\nALAND\nAWATER\ngeometry\n\n\n\n\n0\n48\n01779801\n0400000US48\n48\nTX\nTexas\n00\n676686238592\n18982083586\nPOLYGON ((-998043.807 -568739.971, -997956.109...\n\n\n1\n06\n01779778\n0400000US06\n06\nCA\nCalifornia\n00\n403673296401\n20291770234\nMULTIPOLYGON (((-2066284.899 -204542.622, -205...\n\n\n2\n21\n01779786\n0400000US21\n21\nKY\nKentucky\n00\n102266598312\n2384223544\nPOLYGON ((571924.530 -84268.109, 577745.348 -8...\n\n\n3\n13\n01705317\n0400000US13\n13\nGA\nGeorgia\n00\n149485311347\n4419673221\nPOLYGON ((939223.082 -230281.864, 951008.202 -...\n\n\n4\n55\n01779806\n0400000US55\n55\nWI\nWisconsin\n00\n140292627460\n29343084365\nMULTIPOLYGON (((708320.068 919586.190, 715683....\n\n\n\n\n\n\n\n\n\nCode\nus_rescaled_final = us_rescaled.merge(\n    df4,\n    left_on=[\"GEOID\"],\n    right_on=[\"state\"],\n).drop(\n    columns=[\"state\"]\n)\n\n# Convert CRS\nus_rescaled_final = us_rescaled_final.to_crs(\"EPSG:4326\")\n\n\n\n\nCode\nus_rescaled_final.columns\n\n\nIndex(['STATEFP', 'STATENS', 'GEOIDFQ', 'GEOID', 'STUSPS', 'NAME_x', 'LSAD',\n       'ALAND', 'AWATER', 'geometry', 'NAME_y', 'GEO_ID', 'MedHHInc',\n       'EducTotal', 'EducBelowHighSch', 'EducHighSch', 'EducAssoc', 'EducBach',\n       'TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed',\n       'PopPovertyDetermined', 'PovertyPop', 'PctBach', 'PovertyRate',\n       'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP',\n       'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita'],\n      dtype='object')",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "1. Getting data and data processing"
    ]
  },
  {
    "objectID": "analysis/1.2-Exploratory_Plots_Map.html",
    "href": "analysis/1.2-Exploratory_Plots_Map.html",
    "title": "2. Exploratory Plots [Map]",
    "section": "",
    "text": "3.3 Map\nFinally, we can make a map to see how the variables vary across US states.\nThe map shows that there are large variations in median household income, education levels, unemployment rate, labour force participation rate, labour productivity, population, poverty rate across US states. There are moderate variations in life expectancy, with all states having life expectancy greater than 70. There is small variations in GDP per capita and net exports across states.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "2. Exploratory Plots [Map]"
    ]
  },
  {
    "objectID": "analysis/1.4-PredictiveModelling.html",
    "href": "analysis/1.4-PredictiveModelling.html",
    "title": "4. Predictive Modelling",
    "section": "",
    "text": "With the results, policymakers might be interested in which variables are key predictors of GDP per capita, life expectancy, median household income, and education levels. We can apply a predictive model using decision trees to identify the key predictors. First we select the variables we want to model for.\n\n\n['REALGDPpercapita',\n 'life_expectancy',\n 'MedHHInc',\n 'PctBach',\n 'UnemploymentRate',\n 'LabForParticipationRate',\n 'Labor_Productivity_2023',\n 'TotalPop',\n 'PovertyRate',\n 'netexport']\n\n\nWe then split the data into training and test set.\nWe then run the predictive model for each dependent variable (5.1 to 5.4). The barplot shows ranks the order of importance of each independent variable in predicting the dependent variable.\n\n\n\n\nR^2 scores =  [ 0.42875078  0.7136654  -4.31496545  0.76525778  0.57269267 -3.96447004\n  0.10651717 -0.45416047 -3.25476358  0.60702265]\nScores mean =  -0.8794453102356599\nScore std dev =  1.9846415566046676\n\n\n\n\n0.5397527625306848\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variables that matter most for real GDP per capita is median household income and labor force participation rate. The test score is 0.54 meaning that the model can explain 54% of the variance in real GDP per capita.\n\n\n\n\n\nR^2 scores =  [ 6.95094617e-01  8.85361627e-01  5.46261824e-01  9.60021456e-01\n  9.51224933e-01  8.97905382e-01  4.45589520e-03  3.75819263e-01\n -1.72440959e+01  7.90774467e-01]\nScores mean =  -1.1137176482882758\nScore std dev =  5.384420926046987\n\n\n\n\n0.7472553097289523\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variables that matter most for life expectancy are percentage of bachelor’s degree gradautes, median household income and poverty rate. The test score is 0.75 meaning that the model can explain 75% of the variance in life expectancy.\n\n\n\n\n\nR^2 scores =  [ 0.87021441  0.79626634 -1.81343269  0.94035599  0.91059592  0.85661744\n  0.78405563 -0.84718815  0.59170603  0.44849992]\nScores mean =  0.35376908235570953\nScore std dev =  0.8808696066587679\n\n\n\n\n0.8226485614034753\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variables that matter most for median household income are percentage of bacehlor’s degree graduate, life expectancy, real GDP per capita and poverty rate. The test score is 0.82 meaning that the model can explain 82% of the variance in median household income.\n\n\n\n\n\nR^2 scores =  [ 0.75033831  0.78667313  0.43620382  0.86629284  0.79880008  0.81455566\n  0.08470399 -4.21484583 -0.12530893  0.81489821]\nScores mean =  0.1012311282401515\nScore std dev =  1.4755868640441185\n\n\n\n\n0.7216614717862473\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variable that matter most for education levels is life expectancy. The test score is 0.72 meaning that the model explains 72% of the variance in percentage of bachelor’s degree holders.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "4. Predictive Modelling"
    ]
  },
  {
    "objectID": "analysis/1.4-PredictiveModelling.html#predictive-modelling",
    "href": "analysis/1.4-PredictiveModelling.html#predictive-modelling",
    "title": "4. Predictive Modelling",
    "section": "",
    "text": "With the results, policymakers might be interested in which variables are key predictors of GDP per capita, life expectancy, median household income, and education levels. We can apply a predictive model using decision trees to identify the key predictors. First we select the variables we want to model for.\n\n\n['REALGDPpercapita',\n 'life_expectancy',\n 'MedHHInc',\n 'PctBach',\n 'UnemploymentRate',\n 'LabForParticipationRate',\n 'Labor_Productivity_2023',\n 'TotalPop',\n 'PovertyRate',\n 'netexport']\n\n\nWe then split the data into training and test set.\nWe then run the predictive model for each dependent variable (5.1 to 5.4). The barplot shows ranks the order of importance of each independent variable in predicting the dependent variable.\n\n\n\n\nR^2 scores =  [ 0.42875078  0.7136654  -4.31496545  0.76525778  0.57269267 -3.96447004\n  0.10651717 -0.45416047 -3.25476358  0.60702265]\nScores mean =  -0.8794453102356599\nScore std dev =  1.9846415566046676\n\n\n\n\n0.5397527625306848\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variables that matter most for real GDP per capita is median household income and labor force participation rate. The test score is 0.54 meaning that the model can explain 54% of the variance in real GDP per capita.\n\n\n\n\n\nR^2 scores =  [ 6.95094617e-01  8.85361627e-01  5.46261824e-01  9.60021456e-01\n  9.51224933e-01  8.97905382e-01  4.45589520e-03  3.75819263e-01\n -1.72440959e+01  7.90774467e-01]\nScores mean =  -1.1137176482882758\nScore std dev =  5.384420926046987\n\n\n\n\n0.7472553097289523\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variables that matter most for life expectancy are percentage of bachelor’s degree gradautes, median household income and poverty rate. The test score is 0.75 meaning that the model can explain 75% of the variance in life expectancy.\n\n\n\n\n\nR^2 scores =  [ 0.87021441  0.79626634 -1.81343269  0.94035599  0.91059592  0.85661744\n  0.78405563 -0.84718815  0.59170603  0.44849992]\nScores mean =  0.35376908235570953\nScore std dev =  0.8808696066587679\n\n\n\n\n0.8226485614034753\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variables that matter most for median household income are percentage of bacehlor’s degree graduate, life expectancy, real GDP per capita and poverty rate. The test score is 0.82 meaning that the model can explain 82% of the variance in median household income.\n\n\n\n\n\nR^2 scores =  [ 0.75033831  0.78667313  0.43620382  0.86629284  0.79880008  0.81455566\n  0.08470399 -4.21484583 -0.12530893  0.81489821]\nScores mean =  0.1012311282401515\nScore std dev =  1.4755868640441185\n\n\n\n\n0.7216614717862473\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWe find that the variable that matter most for education levels is life expectancy. The test score is 0.72 meaning that the model explains 72% of the variance in percentage of bachelor’s degree holders.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "4. Predictive Modelling"
    ]
  },
  {
    "objectID": "analysis/1.6-Time_Series.html",
    "href": "analysis/1.6-Time_Series.html",
    "title": "6. Additional-Time Series Analysis",
    "section": "",
    "text": "For the three dependent varaibles real GDP per capita (REALGDPpercapita), median household income (MedHHInc), and percentage of bachelor’s degree graduate (PctBach), I plotted a time-series to analyse the general trend. I did not do for life expectancy as I do not have data for 2022 and 2023.\nEach line is further colour coded by each US states to identify whether there are disparities over time.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe three dependent varaibles real GDP per capita (REALGDPpercapita), median household income (MedHHInc), and percentage of bachelor’s degree graduate (PctBach) appear to be increasing over time from 2019 to 2023.\nPercentage of bachelor’s degree graduate appears to be increasing between 2019 to 2023 for all states. There are clear disparities between states as to the percentage of bachelor’s degree graduate, which is supported by the map plot in our exploratory analysis.\nReal GDP per capita appears to be relatively constant between 2019 to 2023 for most states. This could be due to the economic impact of the Covid-19 pandemic that resulted in fewer economic activities.\nFinally, the gradient of the median household income line looks relatively similar across states, suggesting that all states are experiencing relative growth in median household income.",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "6. Additional-Time Series Analysis"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Final Project: MUSA 5500 Geospatial Data Science in Python",
    "section": "",
    "text": "Final Project: MUSA 5500 Geospatial Data Science in Python\nThis section contains my final project for MUSA 5500 Geospatial Data Science in Python.\nBackground: The United States of America (USA) is a large country comprising 50 states. Each state is not equal, and it would be interesting to analyse and map out the differences in level of development and socioeconomic indicators between each state as if they were 50 sovereign countries. My final project’s research question is inspired by Gapminder. I aim to apply this in the context of US States to answer these questions: how do socioeconomic variables vary across US states? And do they suggest which states are more developed/deprived than others?\nImplication: This analysis is important in understanding the individual state’s performance. It could guide federal decision-making in resource allocation and state-level policymaking to improve the situation of deprived states. Some of the socioeconomic indicators I aim to analyse are GDP per capita, life expectancy, median household income, education levels, variables related to the Human Development Index (World Health Organisation, 2024). Other variables I include are unemployment rate, labour force, participation rate, labour productivity, population, poverty rate, and net exports by state.\nData Sources: Data are sourced from US Census Bureau, Bureau of Economic Analysis, Bureau of Labor Statistics, and Centre for Disease Control and Prevention. The variables analysed are for the year 2023.\nMethodology: I make an interactive bubble graph and map showing the variation in socioeconomic variables across states. I then apply scikit-learn to the data set to identify clusters of states. This would enable us to identify which states are more developed/deprived in the USA. Finally, I apply predictive modelling to identify which variables matter most for the dependent variables: GDP per capita, life expectancy, median household income, and education levels. This could inform policymakers which areas to investigate further, if they seek to improve the level of development in a deprived state.\nReferences: World Health Organisation, 2024. Human development index. [Online] Available at: https://www-who-int.proxy.library.upenn.edu/data/nutrition/nlis/info/human-development-index [Accessed 5 December 2024].",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python"
    ]
  },
  {
    "objectID": "analysis/shortabstract.html",
    "href": "analysis/shortabstract.html",
    "title": "Abstract",
    "section": "",
    "text": "Abstract\nThis section contains my final project for MUSA 5500 Geospatial Data Science in Python.\nBackground: The United States of America (USA) is a large country comprising 50 states. Each state is not equal, and it would be interesting to analyse and map out the differences in level of development and socioeconomic indicators between each state as if they were 50 sovereign countries. My final project’s research question is inspired by Gapminder. I aim to apply this in the context of US States to answer these questions: how do socioeconomic variables vary across US states? And do they suggest which states are more developed/deprived than others?\nImplication: This analysis is important in understanding the individual state’s performance. It could guide federal decision-making in resource allocation and state-level policymaking to improve the situation of deprived states. Some of the socioeconomic indicators I aim to analyse are GDP per capita, life expectancy, median household income, education levels, variables related to the Human Development Index (World Health Organisation, 2024). Other variables I include are unemployment rate, labour force, participation rate, labour productivity, population, poverty rate, and net exports by state.\nData Sources: Data are sourced from US Census Bureau, Bureau of Economic Analysis, Bureau of Labor Statistics, and Centre for Disease Control and Prevention. The variables analysed are for the year 2023.\nMethodology: I make an interactive bubble graph and map showing the variation in socioeconomic variables across states. I then apply scikit-learn to the data set to identify clusters of states. This would enable us to identify which states are more developed/deprived in the USA. Finally, I apply predictive modelling to identify which variables matter most for the dependent variables: GDP per capita, life expectancy, median household income, and education levels. This could inform policymakers which areas to investigate further, if they seek to improve the level of development in a deprived state.\nReferences: World Health Organisation, 2024. Human development index. [Online] Available at: https://www-who-int.proxy.library.upenn.edu/data/nutrition/nlis/info/human-development-index [Accessed 5 December 2024].",
    "crumbs": [
      "Final Project: MUSA 5500 Geospatial Data Science in Python",
      "Abstract"
    ]
  }
]