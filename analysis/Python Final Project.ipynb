{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If US States were individual countries, how do they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the data with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cenpy\n",
    "import cenpy\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 ACS 5-Year Estimate by US States**\n",
    "1. Median Household Income\n",
    "2. Education Levels\n",
    "3. Unemployment rate\n",
    "4. Labour force participation rate\n",
    "5. Population\n",
    "6. Poverty rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding data set\n",
    "available = cenpy.explorer.available()\n",
    "available\n",
    "\n",
    "# We use data from ACS 5 Year, Monthly Export and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = available.filter(regex=\"^ACS\", axis=0)\n",
    "available.filter(regex=\"^ACSDT5Y\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenpy.explorer.explain(\"ACSDT5Y2023\")\n",
    "acs = cenpy.remote.APIConnection('ACSDT5Y2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs.varslike(\n",
    "    pattern=\"B19019_001E\",\n",
    "    by=\"attributes\",\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_variables = [\n",
    "    \"NAME\",\n",
    "    \"GEO_ID\",\n",
    "    \"B19019_001E\", # Total Median Household Income\n",
    "    \"B06009_001E\", # Educational Attainment- Total\n",
    "    \"B06009_002E\", # Educational Attainment- Less than high school graduate\n",
    "    \"B06009_003E\", # Educational Attainment- High school graduate (equivalent)\n",
    "    \"B06009_004E\", # Educational Attainment- Some college or associate's degree\n",
    "    \"B06009_005E\", # Educational Attainment- Bachelor's degree or higher\n",
    "    \"B01003_001E\", # Total Population\n",
    "    \"B23025_001E\", # Total Population above 16\n",
    "    \"B23025_002E\", # Labor Force- Total\n",
    "    \"B23025_005E\", # Labour Force- Unemployed\n",
    "    \"B17020_001E\", # Population for whom poverty is determined\n",
    "    \"B17020_002E\", # Population below poverty level\n",
    "]\n",
    "\n",
    "#1. Median Household Income [x]\n",
    "#2. Education Levels [x]\n",
    "#3. Unemployment rate [x]\n",
    "#4. Labour force participation rate [x]\n",
    "#5. Population [x]\n",
    "#6. Poverty rate [x]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the data for state\n",
    "USA_acs_data = acs.query(\n",
    "    cols=acs_variables,\n",
    "    geo_unit=\"state:*\",\n",
    ")\n",
    "\n",
    "USA_acs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variable in acs_variables:\n",
    "    # Convert all variables EXCEPT for NAME\n",
    "    if variable not in (\"NAME\", \"GEO_ID\"):\n",
    "        USA_acs_data[variable] = USA_acs_data[variable].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_acs_data = USA_acs_data.rename(\n",
    "    columns={\n",
    "        \"B19019_001E\": \"MedHHInc\", # Total Median Household Income\n",
    "        \"B06009_001E\": \"EducTotal\", # Educational Attainment- Total\n",
    "        \"B06009_002E\": \"EducBelowHighSch\", # Educational Attainment- Less than high school graduate\n",
    "        \"B06009_003E\": \"EducHighSch\", # Educational Attainment- High school graduate (equivalent)\n",
    "        \"B06009_004E\": \"EducAssoc\", # Educational Attainment- Some college or associate's degree\n",
    "        \"B06009_005E\": \"EducBach\", # Educational Attainment- Bachelor's degree or higher\n",
    "        \"B01003_001E\": \"TotalPop\", # Total Population\n",
    "        \"B23025_001E\": \"TotalPop16\", # Total Population above 16\n",
    "        \"B23025_002E\": \"LabForTotal\", # Labor Force- Total\n",
    "        \"B23025_005E\": \"Unemployed\", # Labour Force- Unemployed\n",
    "        \"B17020_001E\": \"PopPovertyDetermined\", # Population for whom poverty is determined\n",
    "        \"B17020_002E\": \"PovertyPop\", # Population below poverty level\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Feature engineering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_acs_data['PctBach'] = USA_acs_data['EducBach']/USA_acs_data['EducTotal']\n",
    "USA_acs_data['PovertyRate'] = USA_acs_data['PovertyPop']/USA_acs_data['PopPovertyDetermined']\n",
    "USA_acs_data['UnemploymentRate'] = USA_acs_data['Unemployed']/USA_acs_data['LabForTotal']\n",
    "USA_acs_data['LabForParticipationRate'] = USA_acs_data['LabForTotal']/USA_acs_data['TotalPop16']\n",
    "\n",
    "#Remove Puerto Rico\n",
    "USA_acs_data = USA_acs_data[USA_acs_data['NAME'] != 'Puerto Rico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 International Trade**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenpy.explorer.explain(\"ITMONTHLYEXPORTSSTATENAICS\")\n",
    "US_export = cenpy.remote.APIConnection('ITMONTHLYEXPORTSSTATENAICS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_export.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_export.geographies['fips']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_export_data = US_export.query(\n",
    "    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"],\n",
    "    geo_unit=\"world:*\",\n",
    "    )\n",
    "\n",
    "for variable in [\"YEAR\",\"MONTH\",\"ALL_VAL_YR\"]:\n",
    "    US_export_data[variable] = US_export_data[variable].astype(float)\n",
    "\n",
    "US_export_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for December 2023\n",
    "US_export_2023 = US_export_data[(US_export_data['YEAR'] == 2023) & (US_export_data['MONTH'] == 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Imports*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenpy.explorer.explain(\"ITMONTHLYIMPORTSSTATENAICS\")\n",
    "US_import = cenpy.remote.APIConnection('ITMONTHLYIMPORTSSTATENAICS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_import.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_import_data = US_import.query(\n",
    "    cols=[\"US_STATE\", \"GEO_ID\",\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"],\n",
    "    geo_unit=\"world:*\",\n",
    "    )\n",
    "\n",
    "for variable in [\"YEAR\",\"MONTH\",\"GEN_VAL_YR\"]:\n",
    "    US_import_data[variable] = US_import_data[variable].astype(float)\n",
    "\n",
    "US_import_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for December 2023\n",
    "US_import_2023 = US_import_data[(US_export_data['YEAR'] == 2023) & (US_import_data['MONTH'] == 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Net Export (Export-Import)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join export and import data\n",
    "US_netexport_2023 = US_export_2023.merge(US_import_2023[['US_STATE', 'GEN_VAL_YR']], on='US_STATE', how='left')\n",
    "\n",
    "# Net export\n",
    "US_netexport_2023[\"netexport\"] = US_netexport_2023[\"ALL_VAL_YR\"]-US_netexport_2023[\"GEN_VAL_YR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with states name\n",
    "state_names = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "    'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "    'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "    'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
    "    'DC': 'District of Columbia'\n",
    "}\n",
    "\n",
    "US_netexport_2023['STATE_NAME'] = US_netexport_2023['US_STATE'].map(state_names)\n",
    "\n",
    "# Filter states only\n",
    "US_netexport_2023 = US_netexport_2023.dropna(subset=['STATE_NAME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Bureau of Economic Analysis: Gross Domesitc Product (GDP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beaapi-0.0.2-py3-none-any.whl\n",
    "\n",
    "import beaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "beakey = '60AB5AC4-7ED2-481D-B559-BB6000F924ED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data set available\n",
    "list_of_sets = beaapi.get_data_set_list(beakey)\n",
    "# List of parameters\n",
    "list_of_params = beaapi.get_parameter_list(beakey, 'Regional')\n",
    "# List of parameters values\n",
    "list_of_param_vals = beaapi.get_parameter_values(beakey, 'Regional', 'LineCode',)\n",
    "list_of_param_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bea_tbl = beaapi.get_data(beakey, datasetname='Regional', GeoFips= 'STATE', LineCode= '1', TableName='SAGDP9N', Year='2023')\n",
    "display(bea_tbl.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bea_state_tbl = bea_tbl[bea_tbl['GeoName'].isin(state_names.values())]\n",
    "bea_state_tbl.rename(columns={'DataValue': 'REALGDP'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 Centers for Disease Control and Prevention: Life Expectancy by state**\n",
    "The last available data was 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sodapy\n",
    "import pandas as pd\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc = Socrata(\"data.cdc.gov\", None)\n",
    "life = cdc.get(\"it4f-frdc\", limit=2000)\n",
    "life_df = pd.DataFrame.from_records(life)\n",
    "life_df = life_df[life_df[\"sex\"]==\"Total\"]\n",
    "life_df[\"leb\"] = life_df[\"leb\"].astype(float)\n",
    "life_df.rename(columns={'leb': 'life_expectancy'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5 Bureau of Labor Statistics: Labor Productivity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pdt = pd.read_csv('https://raw.githubusercontent.com/JiaYue-Ong/Python-Final-Project/refs/heads/main/labor-productivity.csv')\n",
    "lab_pdt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pdt_2023 = lab_pdt[lab_pdt['Area'].isin(state_names.values())]\n",
    "lab_pdt_2023 = lab_pdt_2023[[\"Area\",\"2023\"]]\n",
    "lab_pdt_2023.rename(columns={'Area': 'State', '2023': 'Labor_Productivity_2023'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependent variables:\n",
    "1. GDP per capita\n",
    "2. Life expectancy\n",
    "3. Median household income\n",
    "4. Education levels\n",
    "\n",
    "Independent variables\n",
    "1. Unemployment rate\n",
    "2. Labour force participation rate\n",
    "3. Labour Productivity Private non-farm\n",
    "4. Population\n",
    "5. Poverty rate\n",
    "6. Net exports by state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dataset\n",
    "USA_acs_data\n",
    "US_netexport_2023\n",
    "bea_state_tbl\n",
    "life_df\n",
    "lab_pdt_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join ACS and netexport\n",
    "df1 = USA_acs_data.merge(US_netexport_2023[['STATE_NAME', 'netexport']], left_on='NAME', right_on='STATE_NAME', how='left').drop(\n",
    "    columns=[\"STATE_NAME\"]\n",
    ")\n",
    "\n",
    "# Join bea_state_tbl\n",
    "df2 = df1.merge(bea_state_tbl[['GeoName', 'REALGDP']], left_on='NAME', right_on='GeoName', how='left').drop(\n",
    "    columns=[\"GeoName\"]\n",
    ")\n",
    "\n",
    "# Join life_df\n",
    "df3 = df2.merge(life_df[['area', 'life_expectancy']], left_on='NAME', right_on='area', how='left').drop(\n",
    "    columns=[\"area\"]\n",
    ")\n",
    "\n",
    "# Join lab_pdt_2023\n",
    "df4 = df3.merge(lab_pdt_2023, left_on='NAME', right_on='State', how='left').drop(\n",
    "    columns=[\"State\"]\n",
    ")\n",
    "\n",
    "# Create Real GDP per capita\n",
    "df4[\"REALGDPpercapita\"] = df4[\"REALGDP\"]/df4[\"TotalPop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get geographies of US States\n",
    "import pygris\n",
    "from pygris import states\n",
    "from pygris.utils import shift_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us = states(cb = True, resolution = \"20m\", year=2023)\n",
    "us_rescaled = shift_geometry(us)\n",
    "us_rescaled.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data to geography\n",
    "us_rescaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_rescaled_final = us_rescaled.merge(\n",
    "    df4,\n",
    "    left_on=[\"GEOID\"],\n",
    "    right_on=[\"state\"],\n",
    ").drop(\n",
    "    columns=[\"state\"]\n",
    ")\n",
    "\n",
    "# Convert CRS\n",
    "us_rescaled_final = us_rescaled_final.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_rescaled_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all variables\n",
    "variables = ['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita']\n",
    "\n",
    "# Create a list of selected variables for later analysis\n",
    "selected_variables = ['REALGDPpercapita','life_expectancy','MedHHInc','PctBach','UnemploymentRate','LabForParticipationRate', 'Labor_Productivity_2023', 'TotalPop', 'PovertyRate', 'netexport']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = us_rescaled_final[variables].corr()\n",
    "\n",
    "# Plot the correlation matrix using seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Repeated Chart and Bubble Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the selection brush\n",
    "brush = alt.selection_interval()\n",
    "\n",
    "# Repeated chart\n",
    "(\n",
    "    alt.Chart(us_rescaled_final)\n",
    "    .mark_circle()\n",
    "    .encode(\n",
    "        x=alt.X(alt.repeat(\"column\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n",
    "        y=alt.Y(alt.repeat(\"row\"), type=\"quantitative\", scale=alt.Scale(zero=False)),\n",
    "        color=alt.condition(\n",
    "            brush, \"NAME_x:N\", alt.value(\"lightgray\")\n",
    "        ),  # conditional color\n",
    "        tooltip=['NAME_x'] + variables\n",
    "    )\n",
    "    .properties(\n",
    "        width=200,\n",
    "        height=200,\n",
    "    )\n",
    "    .add_params(brush)\n",
    "    .repeat(  # repeat variables across rows and columns\n",
    "        row=variables,\n",
    "        column=variables,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dropdown bindings for both x and y axes\n",
    "dropdown_x = alt.binding_select(\n",
    "    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n",
    "    name='X-axis column '\n",
    ")\n",
    "dropdown_y = alt.binding_select(\n",
    "    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n",
    "    name='Y-axis column '\n",
    ")\n",
    "\n",
    "# Create parameters for x and y axes\n",
    "xcol_param = alt.param(\n",
    "    value='MedHHInc',\n",
    "    bind=dropdown_x\n",
    ")\n",
    "ycol_param = alt.param(\n",
    "    value='MedHHInc',\n",
    "    bind=dropdown_y\n",
    ")\n",
    "\n",
    "chart = alt.Chart(us_rescaled_final).mark_circle().encode(\n",
    "    x=alt.X('x:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n",
    "    y=alt.Y('y:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n",
    "    size='TotalPop:Q',\n",
    "    color='NAME_x:N',\n",
    "    tooltip=['NAME_x'] + variables  # Concatenate NAME_x with the existing variables list\n",
    ").transform_calculate(\n",
    "    x=f'datum[{xcol_param.name}]',\n",
    "    y=f'datum[{ycol_param.name}]'\n",
    ").add_params(\n",
    "    xcol_param,\n",
    "    ycol_param,\n",
    ").properties(width=800, height=800)\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dropdown bindings for both x and y axes\n",
    "dropdown_x = alt.binding_select(\n",
    "    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n",
    "    name='X-axis column '\n",
    ")\n",
    "dropdown_y = alt.binding_select(\n",
    "    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n",
    "    name='Y-axis column '\n",
    ")\n",
    "dropdown_size = alt.binding_select(\n",
    "    options=['MedHHInc','TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed','PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023'],\n",
    "    name='Bubble Size '\n",
    ")\n",
    "\n",
    "# Create parameters for x and y axes\n",
    "xcol_param = alt.param(\n",
    "    value='MedHHInc',\n",
    "    bind=dropdown_x\n",
    ")\n",
    "ycol_param = alt.param(\n",
    "    value='MedHHInc',\n",
    "    bind=dropdown_y\n",
    ")\n",
    "size_param = alt.param(\n",
    "    value='MedHHInc',\n",
    "    bind=dropdown_size\n",
    ")\n",
    "\n",
    "chart2 = alt.Chart(us_rescaled_final).mark_circle().encode(\n",
    "    x=alt.X('x:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n",
    "    y=alt.Y('y:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n",
    "    size=alt.Size('size:Q', scale=alt.Scale(zero=False, domain='unaggregated')).title(''),\n",
    "    color='NAME_x:N',\n",
    "    tooltip=['NAME_x'] + variables  # Concatenate NAME_x with the existing variables list\n",
    ").transform_calculate(\n",
    "    x=f'datum[{xcol_param.name}]',\n",
    "    y=f'datum[{ycol_param.name}]',\n",
    "    size=f'datum[{size_param.name}]'\n",
    ").add_params(\n",
    "    xcol_param,\n",
    "    ycol_param,\n",
    "    size_param,\n",
    ").properties(width=800, height=800)\n",
    "\n",
    "chart2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopandas hvplot panel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import panel as pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from wide to long data\n",
    "us_rescaled_final_long = pd.melt(us_rescaled_final,\n",
    "                                 id_vars = ['STATEFP', 'STATENS', 'GEOIDFQ', 'GEOID', 'STUSPS', 'NAME_x', 'LSAD','ALAND', 'AWATER', 'geometry', 'NAME_y', 'GEO_ID'],\n",
    "                                 value_vars=['MedHHInc', 'EducTotal', 'EducBelowHighSch', 'EducHighSch', 'EducAssoc', 'EducBach', 'TotalPop', 'TotalPop16', 'LabForTotal', 'Unemployed', 'PopPovertyDetermined', 'PovertyPop', 'PctBach', 'PovertyRate', 'UnemploymentRate', 'LabForParticipationRate', 'netexport', 'REALGDP', 'life_expectancy', 'Labor_Productivity_2023', 'REALGDPpercapita']\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_rescaled_final_long.hvplot(\n",
    "    c=\"value\",\n",
    "    dynamic=False,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    geo=True,\n",
    "    cmap=\"viridis\",\n",
    "    groupby=\"variable\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_rescaled_final_scaled = scaler.fit_transform(us_rescaled_final[selected_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_rescaled_final_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters to try out\n",
    "n_clusters = list(range(2, 10))\n",
    "\n",
    "# Run kmeans for each value of k\n",
    "inertias = []\n",
    "for k in n_clusters:\n",
    "    \n",
    "    # Initialize and run\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(us_rescaled_final_scaled)\n",
    "    \n",
    "    # Save the \"inertia\"\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Initialize the knee algorithm\n",
    "kn = KneeLocator(n_clusters, inertias, curve='convex', direction='decreasing')\n",
    "\n",
    "# Print out the knee \n",
    "print(kn.knee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, n_init=10)\n",
    "\n",
    "# Perform the fit\n",
    "kmeans.fit(us_rescaled_final_scaled)\n",
    "\n",
    "# Extract the labels\n",
    "us_rescaled_final['label'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average feature per cluster\n",
    "us_rescaled_final.groupby('label', as_index=False).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_rescaled_final.groupby(\"label\", as_index=False)[\n",
    "    selected_variables\n",
    "].mean().sort_values(by=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map plot clusters\n",
    "us_rescaled_final.hvplot(\n",
    "    c=\"label\",\n",
    "    dynamic=False,\n",
    "    width=1000,\n",
    "    height=1000,\n",
    "    geo=True,\n",
    "    cmap=\"viridis\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_modelling = us_rescaled_final[selected_variables + [\"geometry\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 70/30\n",
    "train_set, test_set = train_test_split(usa_modelling, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 GDP per capita against variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target labels: log of sale price\n",
    "y_GDPtrain = np.log(train_set[\"REALGDPpercapita\"])\n",
    "y_GDPtest = np.log(test_set[\"REALGDPpercapita\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features\n",
    "GDPfeature_cols = [\n",
    "    'life_expectancy',\n",
    "    'MedHHInc',\n",
    "    'PctBach',\n",
    "    'UnemploymentRate',\n",
    "    'LabForParticipationRate',\n",
    "    'Labor_Productivity_2023',\n",
    "    'TotalPop',\n",
    "    'PovertyRate',\n",
    "    'netexport',\n",
    "]\n",
    "X_GDPtrain = train_set[GDPfeature_cols].values\n",
    "X_GDPtest = test_set[GDPfeature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random forest pipeline\n",
    "forest_pipeline = make_pipeline(\n",
    "    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "# Run the 10-fold cross validation\n",
    "GDPscores = cross_val_score(\n",
    "    forest_pipeline,\n",
    "    X_GDPtrain,\n",
    "    y_GDPtrain,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "# Report\n",
    "print(\"R^2 scores = \", GDPscores)\n",
    "print(\"Scores mean = \", GDPscores.mean())\n",
    "print(\"Score std dev = \", GDPscores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "forest_pipeline.fit(X_GDPtrain, y_GDPtrain)\n",
    "\n",
    "# What's the test score?\n",
    "forest_pipeline.score(X_GDPtest, y_GDPtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the regressor from the pipeline\n",
    "forest_model = forest_pipeline[\"randomforestregressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of importances\n",
    "importanceGDP = pd.DataFrame(\n",
    "    {\"Feature\": GDPfeature_cols, \"Importance\": forest_model.feature_importances_}\n",
    ").sort_values(\"Importance\")\n",
    "\n",
    "\n",
    "importanceGDP.hvplot.barh(x=\"Feature\", y=\"Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Life Expectancy against variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target labels: log of sale price\n",
    "y_LEtrain = np.log(train_set[\"life_expectancy\"])\n",
    "y_LEtest = np.log(test_set[\"life_expectancy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features\n",
    "LEfeature_cols = [\n",
    "    'REALGDPpercapita',\n",
    "    'MedHHInc',\n",
    "    'PctBach',\n",
    "    'UnemploymentRate',\n",
    "    'LabForParticipationRate',\n",
    "    'Labor_Productivity_2023',\n",
    "    'TotalPop',\n",
    "    'PovertyRate',\n",
    "    'netexport',\n",
    "]\n",
    "X_LEtrain = train_set[LEfeature_cols].values\n",
    "X_LEtest = test_set[LEfeature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random forest pipeline\n",
    "LEforest_pipeline = make_pipeline(\n",
    "    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "# Run the 10-fold cross validation\n",
    "LEscores = cross_val_score(\n",
    "    forest_pipeline,\n",
    "    X_LEtrain,\n",
    "    y_LEtrain,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "# Report\n",
    "print(\"R^2 scores = \", LEscores)\n",
    "print(\"Scores mean = \", LEscores.mean())\n",
    "print(\"Score std dev = \", LEscores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "LEforest_pipeline.fit(X_LEtrain, y_LEtrain)\n",
    "\n",
    "# What's the test score?\n",
    "LEforest_pipeline.score(X_LEtest, y_LEtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the regressor from the pipeline\n",
    "LEforest_model = LEforest_pipeline[\"randomforestregressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of importances\n",
    "importanceLE = pd.DataFrame(\n",
    "    {\"Feature\": LEfeature_cols, \"Importance\": LEforest_model.feature_importances_}\n",
    ").sort_values(\"Importance\")\n",
    "\n",
    "\n",
    "importanceLE.hvplot.barh(x=\"Feature\", y=\"Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Median Household Income against variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target labels: log of sale price\n",
    "y_HHINCtrain = np.log(train_set[\"MedHHInc\"])\n",
    "y_HHINCtest = np.log(test_set[\"MedHHInc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features\n",
    "HHINCfeature_cols = [\n",
    "    'REALGDPpercapita',\n",
    "    'life_expectancy',\n",
    "    'PctBach',\n",
    "    'UnemploymentRate',\n",
    "    'LabForParticipationRate',\n",
    "    'Labor_Productivity_2023',\n",
    "    'TotalPop',\n",
    "    'PovertyRate',\n",
    "    'netexport',\n",
    "]\n",
    "X_HHINCtrain = train_set[HHINCfeature_cols].values\n",
    "X_HHINCtest = test_set[HHINCfeature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random forest pipeline\n",
    "HHINCforest_pipeline = make_pipeline(\n",
    "    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "# Run the 10-fold cross validation\n",
    "HHINCscores = cross_val_score(\n",
    "    forest_pipeline,\n",
    "    X_HHINCtrain,\n",
    "    y_HHINCtrain,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "# Report\n",
    "print(\"R^2 scores = \", HHINCscores)\n",
    "print(\"Scores mean = \", HHINCscores.mean())\n",
    "print(\"Score std dev = \", HHINCscores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "HHINCforest_pipeline.fit(X_HHINCtrain, y_HHINCtrain)\n",
    "\n",
    "# What's the test score?\n",
    "HHINCforest_pipeline.score(X_HHINCtest, y_HHINCtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the regressor from the pipeline\n",
    "HHINCforest_model = HHINCforest_pipeline[\"randomforestregressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of importances\n",
    "importanceHHINC = pd.DataFrame(\n",
    "    {\"Feature\": HHINCfeature_cols, \"Importance\": HHINCforest_model.feature_importances_}\n",
    ").sort_values(\"Importance\")\n",
    "\n",
    "\n",
    "importanceHHINC.hvplot.barh(x=\"Feature\", y=\"Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Education levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target labels: log of sale price\n",
    "y_PctBachtrain = np.log(train_set[\"PctBach\"])\n",
    "y_PctBachtest = np.log(test_set[\"PctBach\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features\n",
    "PctBachfeature_cols = [\n",
    "    'REALGDPpercapita',\n",
    "    'life_expectancy',\n",
    "    'MedHHInc',\n",
    "    'UnemploymentRate',\n",
    "    'LabForParticipationRate',\n",
    "    'Labor_Productivity_2023',\n",
    "    'TotalPop',\n",
    "    'PovertyRate',\n",
    "    'netexport',\n",
    "]\n",
    "X_PctBachtrain = train_set[PctBachfeature_cols].values\n",
    "X_PctBachtest = test_set[PctBachfeature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random forest pipeline\n",
    "PctBachforest_pipeline = make_pipeline(\n",
    "    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n",
    ")\n",
    "\n",
    "# Run the 10-fold cross validation\n",
    "PctBachscores = cross_val_score(\n",
    "    forest_pipeline,\n",
    "    X_PctBachtrain,\n",
    "    y_PctBachtrain,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "# Report\n",
    "print(\"R^2 scores = \", PctBachscores)\n",
    "print(\"Scores mean = \", PctBachscores.mean())\n",
    "print(\"Score std dev = \", PctBachscores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "PctBachforest_pipeline.fit(X_PctBachtrain, y_PctBachtrain)\n",
    "\n",
    "# What's the test score?\n",
    "PctBachforest_pipeline.score(X_PctBachtest, y_PctBachtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the regressor from the pipeline\n",
    "PctBachforest_model = PctBachforest_pipeline[\"randomforestregressor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of importances\n",
    "importancePctBach = pd.DataFrame(\n",
    "    {\"Feature\": PctBachfeature_cols, \"Importance\": PctBachforest_model.feature_importances_}\n",
    ").sort_values(\"Importance\")\n",
    "\n",
    "\n",
    "importancePctBach.hvplot.barh(x=\"Feature\", y=\"Importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musa-550-fall-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
